#!/bin/env python3

import sys
import argparse
import json
import re
import subprocess
from pathlib import Path
from datetime import datetime, timezone, timedelta
from time import sleep
import urllib.request, urllib.parse
from bs4 import BeautifulSoup

firefox_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:95.0) Gecko/20100101 Firefox/95.0'
firefox_headers = {'User-Agent': firefox_agent}

user_agent = 'Lavf/58.134.100'
headers = {'User-Agent': user_agent}

# Parse arguments
parser = argparse.ArgumentParser(description='Download/archive an NBC video stream.')
parser.add_argument('url',metavar='url',nargs='*',default='',
                    help='URL(s) of video to download.')
parser.add_argument('-N','--nocolor', action='store_true', dest='nocolor',
                    help='Do not use terminal colors.')
args = parser.parse_args()

# Define terminal colors
class bcolors:
  HEADER = '\033[95m'
  OKBLUE = '\033[94m'
  OKGREEN = '\033[92m'
  WARNING = '\033[93m'
  FAIL = '\033[91m'
  ENDC = '\033[0m'
  BOLD = '\033[1m'
  UNDERLINE = '\033[4m'
# Remove colors from terminal if option is selected
if args.nocolor:
  bcolors.HEADER = ''
  bcolors.OKBLUE = ''
  bcolors.OKGREEN = ''
  bcolors.WARNING = ''
  bcolors.FAIL = ''
  bcolors.ENDC = ''
  bcolors.BOLD = ''
  bcolors.UNDERLINE = ''

def quarter(mm):
  if mm in ('01','02','03'):
    return 'Q1'
  elif mm in ('04','05','06'):
    return 'Q2'
  elif mm in ('07','08','09'):
    return 'Q3'
  elif mm in ('10','11','12'):
    return 'Q4'
  else:
    return None

def parseUrl(raw_url):
  try:
    ld = re.split(r'(?x)^(?:\#)?(?:http)?(?:s)?:?\/\/((?:www\.)?(?:nbcnews)\.com)\/([^\/\r\n]+)\/([^\/\r\n]+)\/(([^\/\r\n]+)-([^\/\r\n]+))$',raw_url)
    return {'url':'https://{}/{}/{}/{}'.format(ld[1],ld[2],ld[3],ld[4]),
            'server':ld[1],
            'name':ld[5],
            'id':ld[6]}
  except IndexError:
    try:
      ld = re.split(r'(?x)^(?:\#)?(?:http)?(?:s)?:?\/\/((?:www\.)?(?:nbcnews)\.com)\/([^\/\r\n]+)\/(([^\/\r\n]+)-([^\/\r\n]+))$',raw_url)
      return {'url':'https://{}/{}/{}'.format(ld[1],ld[2],ld[3]),
              'server':ld[1],
              'name':ld[4],
              'id':ld[5]}
    except IndexError:
      try:
        ld = re.split(r'(?x)^(?:\#)?(?:http)?(?:s)?:?\/\/((?:www\.)?(?:nbc)\.com)\/([^\/\r\n]+)\/([^\/\r\n]+)\/(([^\/\r\n]+)\/([^\/\r\n]+))$',raw_url)
        return {'url':'https://{}/{}/{}/{}'.format(ld[1],ld[2],ld[3],ld[4]),
                'server':ld[1],
                'name':ld[5],
                'id':ld[6]}
      except IndexError:
        return None


def initializeDownload(x):

  # Download webpage
  req = urllib.request.Request(x['url'], data=None, headers=headers)
  with urllib.request.urlopen(req) as response:
    webpage = response.read()
    soup = BeautifulSoup(webpage,features='html.parser')
  print(soup.title.string)
  #with open('file.txt'.format(dir),'wb') as f:
  #  f.write(webpage)

  if 'nbcnews.com' in x['server']:
    # Get NEXT_DATA (JSON)
    nextdata = soup.find(attrs={'id':'__NEXT_DATA__'}).string
    jsondata = json.loads(nextdata)
    videoAssets = jsondata['props']['initialState']['video']['current']['videoAssets']
    videoAssets.sort(key=lambda x: x['bitrate'],reverse=True)

    #print(json.dumps(videoAssets,indent=2))

    dateBroadcast = jsondata['props']['initialState']['video']['current']['dateBroadcast']
  
    # This converts the UTC to Central Time in a rudimentary way
    # It would be better to use pyzt, but this calculation doesnt need to be
    # really precise.  I am OK with it being off by an hour or two.
    broadcastDate = datetime.strptime(dateBroadcast, '%a %b %d %Y %H:%M:%S %Z%z (UTC)')
    broadcastDateCT = broadcastDate.replace(tzinfo=timezone(timedelta(seconds=-21600), 'CT')) - timedelta(seconds=21600)
  
    # Determine the date code for the program
    x['datecode'] = broadcastDateCT.strftime('%y%m%d')

  elif 'nbc.com' in x['server']:
    # Get PRELOAD (JSON)
    preload = json.loads(re.split(r'^PRELOAD\=(.+)$',soup.find('script',text=re.compile(r'^PRELOAD\=(.+)$')).string)[1])
    preload_pages=preload['pages'][list(preload['pages'].keys())[0]]
    #with open('preload.txt'.format(dir),'w') as f:
    #  f.write(json.dumps(preload,indent=2))

    # Get LD+JSON (JSON)
    ldjson = json.loads(soup.find('script',attrs={'type':re.compile('json')}).string)
    #with open('ldjson.txt'.format(dir),'w') as f:
    #  f.write(json.dumps(ldjson,indent=2))

    # Get MPX (JSON)
    mpx = re.split(r'(MPX:.+?}})',webpage.decode('utf-8'))[1]
    quoted=False
    quoted2=False
    mpx2 = '{'
    for i in range(len(mpx)):
      if mpx[i] == '"':
        quoted = not(quoted)
      if quoted or mpx[i] == '"':
        mpx2+=mpx[i]
      else:
        if not quoted2 and mpx[i] not in ':,{}':
          mpx2+='"'+mpx[i]
          quoted2=True
        elif quoted2 and mpx[i] in ':,{}':
          mpx2+='"'+mpx[i]
          quoted2=False
        elif quoted2 and mpx[i] not in ':,{}':
          mpx2+=mpx[i]
        elif not quoted2 and mpx[i] in ':,{}':
          mpx2+=mpx[i]
    mpx2 += '}'
    mpxjson = json.loads(mpx2)['MPX']
    #with open('mpxjson.txt'.format(dir),'w') as f:
    #  f.write(json.dumps(mpxjson,indent=2))
          
    # Broadcast date
    try:
      dateBroadcast = preload_pages['metadata']['airDate']
    except KeyError:
      dateBroadcast = ldjson['video']['uploadDate']

    # This converts the UTC to Central Time in a rudimentary way
    # It would be better to use pyzt, but this calculation doesnt need to be
    # really precise.  I am OK with it being off by an hour or two.
    broadcastDate = datetime.strptime(dateBroadcast, '%Y-%m-%dT%H:%M:%S.%fZ')
    broadcastDateCT = broadcastDate.replace(tzinfo=timezone(timedelta(seconds=-21600), 'CT')) - timedelta(seconds=21600)

    # Determine the date code for the program
    x['datecode'] = broadcastDateCT.strftime('%y%m%d')

####################
### MAIN PROGRAM ###
####################

# Get the raw list of input URLs from the command line and read any URL files provided.
try:
  url_list = []
  for u in args.url:
    if Path(u).is_file():
      try:
        with open(u) as f:
          v = f.read().splitlines()
        url_list.extend(v)
      except UnicodeDecodeError:
        url_list.extend([u])
    else:
      url_list.extend([u])
  if len(url_list)==0:
    url_list = None
except IndexError:
  url_list = None

# Parse the URLs and kick out any that could not be parsed.
if url_list == None:
  parser.print_help()
else:
  items = []
  for url in url_list:
    i = parseUrl(url)
    if i != None:
      items.extend([i])
    else:
      print("{}ERROR: No rules to interpret `{}`.{}".format(bcolors.FAIL,url,bcolors.ENDC))

# Download the URLs
for item in items:
  print("{}Downloading `{}`...{}".format(bcolors.OKBLUE,item['url'],bcolors.ENDC))
  initializeDownload(item)

# Debug output
print(json.dumps(items,indent=2))

retcd=0
sys.exit(retcd)
