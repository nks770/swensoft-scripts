#!/bin/env python3

import sys
import argparse
import json
import re
import subprocess
from pathlib import Path
from datetime import datetime, timezone, timedelta
from time import sleep
import urllib.request, urllib.parse
from bs4 import BeautifulSoup

firefox_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:95.0) Gecko/20100101 Firefox/95.0'
firefox_headers = {'User-Agent': firefox_agent}

user_agent = 'Lavf/58.134.100'
headers = {'User-Agent': user_agent}

# Parse arguments
parser = argparse.ArgumentParser(description='Download/archive an NBC video stream.')
parser.add_argument('url',metavar='url',nargs='*',default='',
                    help='URL(s) of video to download.')
parser.add_argument('-N','--nocolor', action='store_true', dest='nocolor',
                    help='Do not use terminal colors.')
args = parser.parse_args()

# Define terminal colors
class bcolors:
  HEADER = '\033[95m'
  OKBLUE = '\033[94m'
  OKGREEN = '\033[92m'
  WARNING = '\033[93m'
  FAIL = '\033[91m'
  ENDC = '\033[0m'
  BOLD = '\033[1m'
  UNDERLINE = '\033[4m'
# Remove colors from terminal if option is selected
if args.nocolor:
  bcolors.HEADER = ''
  bcolors.OKBLUE = ''
  bcolors.OKGREEN = ''
  bcolors.WARNING = ''
  bcolors.FAIL = ''
  bcolors.ENDC = ''
  bcolors.BOLD = ''
  bcolors.UNDERLINE = ''

def quarter(mm):
  if mm in ('01','02','03'):
    return 'Q1'
  elif mm in ('04','05','06'):
    return 'Q2'
  elif mm in ('07','08','09'):
    return 'Q3'
  elif mm in ('10','11','12'):
    return 'Q4'
  else:
    return None

def parseUrl(raw_url):
  try:
    ld = re.split(r'(?x)^(?:\#)?(?:http)?(?:s)?:?\/\/((?:www\.)?(?:nbcnews)\.com)\/([^\/\r\n]+)\/([^\/\r\n]+)\/(([^\/\r\n]+)-([^\/\r\n]+))$',raw_url)
    return {'url':'https://{}/{}/{}/{}'.format(ld[1],ld[2],ld[3],ld[4]),
            'server':ld[1],
            'name':ld[5],
            'id':ld[6]}
  except IndexError:
    try:
      ld = re.split(r'(?x)^(?:\#)?(?:http)?(?:s)?:?\/\/((?:www\.)?(?:nbcnews)\.com)\/([^\/\r\n]+)\/(([^\/\r\n]+)-([^\/\r\n]+))$',raw_url)
      return {'url':'https://{}/{}/{}'.format(ld[1],ld[2],ld[3]),
              'server':ld[1],
              'name':ld[4],
              'id':ld[5]}
    except IndexError:
      try:
        ld = re.split(r'(?x)^(?:\#)?(?:http)?(?:s)?:?\/\/((?:www\.)?(?:nbc)\.com)\/([^\/\r\n]+)\/([^\/\r\n]+)\/(([^\/\r\n]+)\/([^\/\r\n]+))$',raw_url)
        return {'url':'https://{}/{}/{}/{}'.format(ld[1],ld[2],ld[3],ld[4]),
                'server':ld[1],
                'name':ld[5],
                'id':ld[6]}
      except IndexError:
        return None


def initializeDownload(x):

  # Download webpage
  req = urllib.request.Request(x['url'], data=None, headers=headers)
  try:
    with urllib.request.urlopen(req) as response:
      webpage = response.read()
      soup = BeautifulSoup(webpage,features='html.parser')
    #print(soup.title.string)
    #with open('file.txt','wb') as f:
    #  f.write(webpage)
  except urllib.error.HTTPError:
    webpage = None
    soup = None

  #############################
  ### RULES FOR NBCNEWS.COM ###
  #############################

  if 'nbcnews.com' in x['server']:
    #####################
    # Get JSON Metadata #
    #####################
    # Get NEXT_DATA (JSON)
    try:
      nextdata = soup.find(attrs={'id':'__NEXT_DATA__'}).string
      jsondata = json.loads(nextdata)
      videoAssets = jsondata['props']['initialState']['video']['current']['videoAssets']
      videoAssets.sort(key=lambda x: x['bitrate'],reverse=True)
      #print(json.dumps(videoAssets,indent=2))
    except AttributeError:
      jsondata = None
      videoAssets = []

    ####################
    # Video asset URLs #
    ####################
    if len(videoAssets)==0:
      x['asset'] = None
      x['asset_ext'] = None
    else:
      assetUrl = re.split(r'^([^\?]+)\?(.+)$',videoAssets[0]['publicUrl'])
      if len(assetUrl)>2:
        x['asset'] = assetUrl[1]
        x['asset_ext'] = assetUrl[2]
      else:
        x['asset'] = videoAssets[0]['publicUrl']
        x['asset_ext'] = None


    ##################
    # Broadcast Date #
    ##################
    try:
      dateBroadcast = jsondata['props']['initialState']['video']['current']['dateBroadcast']
  
      # This converts the UTC to Central Time in a rudimentary way
      # It would be better to use pyzt, but this calculation doesnt need to be
      # really precise.  I am OK with it being off by an hour or two.
      broadcastDate = datetime.strptime(dateBroadcast, '%a %b %d %Y %H:%M:%S %Z%z (UTC)')
      broadcastDateCT = broadcastDate.replace(tzinfo=timezone(timedelta(seconds=-21600), 'CT')) - timedelta(seconds=21600)
  
      # Determine the date code for the program
      x['datecode'] = broadcastDateCT.strftime('%y%m%d')
    except TypeError:
      x['datecode'] = None

    ########
    # GUID #
    ########
    try:
      x['guid'] = jsondata['props']['initialState']['video']['current']['mpxMetadata']['guid']
    except TypeError:
      x['guid'] = x['id']

    ###############
    # Video Title #
    ###############
    try:
      x['title'] = jsondata['props']['initialState']['video']['current']['headline']['primary']
    except TypeError:
      x['title'] = None

    #####################
    # Video Description #
    #####################
    try:
      x['description'] = jsondata['props']['initialState']['video']['current']['description']['primary']
    except TypeError:
      x['description'] = None

    #############
    # Thumbnail #
    #############
    try:
      x['thumbnail'] = {}
      x['thumbnail']['url'] = jsondata['props']['initialState']['video']['current']['primaryImage']['url']['primary']
      x['thumbnail']['filename'] = re.split(r'^([^\?]+)\/([^\?]+)(.*)$',x['thumbnail']['url'])[2]
    except (KeyError, IndexError, TypeError) as e:
      x['thumbnail'] = None

    ############
    # Captions #
    ############
    try:
      x['captions'] = {}
      closedCaptioning = jsondata['props']['initialState']['video']['current']['closedCaptioning']
      x['captions']['url'] = urllib.request.urlopen(closedCaptioning['srt']).geturl()
      x['captions']['filename'] = re.split(r'^([^\?]+)\/([^\?]+)(.*)$',x['captions']['url'])[2]
    except (KeyError, IndexError, TypeError, AttributeError, urllib.error.HTTPError) as e:
      x['captions'] = None


  #########################
  ### RULES FOR NBC.COM ###
  #########################

  elif 'nbc.com' in x['server']:

    #####################
    # Get JSON Metadata #
    #####################
    # Get PRELOAD (JSON)
    try:
      preload = json.loads(re.split(r'^PRELOAD\=(.+)$',soup.find('script',text=re.compile(r'^PRELOAD\=(.+)$')).string)[1])
      preload_pages=preload['pages'][list(preload['pages'].keys())[0]]
      #with open('preload.txt','w') as f:
      #  f.write(json.dumps(preload,indent=2))
    except AttributeError:
      preload = None
      preload_pages = None

    # Get LD+JSON (JSON)
    try:
      ldjson = json.loads(soup.find('script',attrs={'type':re.compile('json')}).string)
      #with open('ldjson.txt','w') as f:
      #  f.write(json.dumps(ldjson,indent=2))
    except AttributeError:
      ldjson = None

    # Get MPX (JSON)
    try:
      mpx = re.split(r'(MPX:.+?}})',webpage.decode('utf-8'))[1]
      quoted=False
      quoted2=False
      mpx2 = '{'
      for i in range(len(mpx)):
        if mpx[i] == '"':
          quoted = not(quoted)
        if quoted or mpx[i] == '"':
          mpx2+=mpx[i]
        else:
          if not quoted2 and mpx[i] not in ':,{}':
            mpx2+='"'+mpx[i]
            quoted2=True
          elif quoted2 and mpx[i] in ':,{}':
            mpx2+='"'+mpx[i]
            quoted2=False
          elif quoted2 and mpx[i] not in ':,{}':
            mpx2+=mpx[i]
          elif not quoted2 and mpx[i] in ':,{}':
            mpx2+=mpx[i]
      mpx2 += '}'
      mpxjson = json.loads(mpx2)['MPX']
      #with open('mpxjson.txt','w') as f:
      #  f.write(json.dumps(mpxjson,indent=2))
    except AttributeError:
      mpxjson = None
          
    ####################
    # Video asset URLs #
    ####################
    # Get the player link and download it
    try:
      player_link = '{}/p/{}/{}/select/media/guid/{}/{}'.format(mpxjson['domain'],mpxjson['pid'],mpxjson['playerName'],mpxjson['id'],x['id'])
      req = urllib.request.Request(player_link, data=None, headers=headers)
      with urllib.request.urlopen(req) as response:
        player_data = response.read()
        player_soup = BeautifulSoup(player_data,features='html.parser')
      #with open('player.txt','wb') as f:
      #  f.write(player_data)
    except TypeError:
      player_data = None
      player_soup = None

    # Try a couple of methods to get the asset link from the player data
    try:
      asset_method1 = player_soup.find('link',attrs={'rel':'alternate','type':re.compile('smil')})['href']
    except (AttributeError, TypeError) as e:
      asset_method1 = None
    try:
      asset_method2 = re.findall(r'(?i:tp\:releaseurl)=\"([^\"]+)\"',player_data.decode('utf-8'))[0]
    except (AttributeError, IndexError) as e:
      asset_method2 = None

    if asset_method1 != None:
      asset_url = asset_method1
    else:
      asset_url = asset_method2

    if asset_method1 != None and asset_method2 != None and asset_method1 != asset_method2:
      raise Exception("ERROR: Links different.\nlink1={}\nlink2={}.".format(asset_method1,asset_method2))

    # Clean up the asset link
    if asset_url == None:
      x['asset'] = None
      x['asset_ext'] = None
    else:
      asset_url_split = re.split(r'^([^\?]+)\?(.+)$',asset_url)
      if len(asset_url_split)>2:
        x['asset'] = asset_url_split[1]
        x['asset_ext'] = asset_url_split[2]
      else:
        x['asset'] = asset_url
        x['asset_ext'] = None

    ##################
    # Broadcast Date #
    ##################
    try:
      try:
        dateBroadcast = preload_pages['metadata']['airDate']
      except (KeyError, TypeError) as e:
        dateBroadcast = ldjson['video']['uploadDate']

      # This converts the UTC to Central Time in a rudimentary way
      # It would be better to use pyzt, but this calculation doesnt need to be
      # really precise.  I am OK with it being off by an hour or two.
      broadcastDate = datetime.strptime(dateBroadcast, '%Y-%m-%dT%H:%M:%S.%fZ')
      broadcastDateCT = broadcastDate.replace(tzinfo=timezone(timedelta(seconds=-21600), 'CT')) - timedelta(seconds=21600)

      # Determine the date code for the program
      x['datecode'] = broadcastDateCT.strftime('%y%m%d')
    except TypeError:
      x['datecode'] = None

    ########
    # GUID #
    ########
    x['guid'] = x['id']

    ###############
    # Video Title #
    ###############
    try:
      x['title'] = ldjson['video']['name']
    except (KeyError, TypeError) as e:
      try:
        x['title'] = preload_pages['metadata']['secondaryTitle']
      except (KeyError, TypeError) as e:
        try:
          x['title'] = player_soup.find('meta',attrs={'property':'og:title'})['content']
        except (AttributeError, TypeError) as e:
          x['title'] = None

    #####################
    # Video Description #
    #####################
    try:
      x['description'] = preload_pages['metadata']['ariaLabel']
    except (KeyError, TypeError) as e:
      try:
        x['description'] = preload_pages['metadata']['title']
      except (KeyError, TypeError) as e:
        try:
          x['description'] = preload_pages['metadata']['description']
        except (KeyError, TypeError) as e:
          try:
            x['description'] = ldjson['video']['description']
          except (KeyError, TypeError) as e:
            try:
              x['description'] = player_soup.find('meta',attrs={'property':'og:description'})['content']
            except (AttributeError, TypeError) as e:
              x['description'] = None

    #############
    # Thumbnail #
    #############
    try:
      x['thumbnail'] = {}
      try:
        x['thumbnail']['url'] = ldjson['video']['thumbnailUrl']
      except KeyError:
        try:
          x['thumbnail']['url'] = preload_pages['metadata']['image']
        except KeyError:
          x['thumbnail']['url'] = player_soup.find('meta',attrs={'property':'og:image'})['content']
      x['thumbnail']['filename'] = re.split(r'^([^\?]+)\/([^\?]+)(.*)$',x['thumbnail']['url'])[2]
    except (KeyError, IndexError, TypeError) as e:
      x['thumbnail'] = None

    ############
    # Captions #
    ############
    previewUrl = '{}?format=preview&formats=MPEG-DASH+widevine,M3U+appleHlsEncryption,M3U+none,MPEG-DASH+none,MPEG4,MP3'.format(x['asset'])
    try:
      req = urllib.request.Request(previewUrl, data=None, headers=headers)
      with urllib.request.urlopen(req) as response:
        preview = response.read()
      #with open('preview.txt','wb') as f:
      #  f.write(preview)
      preview_json = json.loads(preview.decode('utf-8'))
    except (ValueError, urllib.error.HTTPError) as e:
      preview_json = None

    try:
      x['captions'] = {}
      for cap in preview_json['captions']:
        if cap['lang'] == 'en':
          x['captions']['url'] = cap['src']
      x['captions']['filename'] = re.split(r'^([^\?]+)\/([^\?]+)(.*)$',x['captions']['url'])[2]
    except (KeyError, IndexError, TypeError, AttributeError) as e:
      x['captions'] = None

  # Initialize an empty data list
  x['data'] = {'mp4':None,'m3u':None}

  ############
  ## Status ##
  ############
  if x['asset'] != None and x['title'] != None and x['guid'] != None and x['datecode'] != None:
    x['status'] = 'INITIALIZE_OK'
  else:
    x['status'] = 'INITIALIZE_FAIL'



#####################
### GET MP4 LINKS ###
#####################

def getMP4Link(x):
  try:
    url = '{}?mbr=true&player=%5Bv2%5D%20OneApp%20-%20PDK6%20NBC.com&format=SMIL&formats=MPEG4'.format(x['asset'])
    req = urllib.request.Request(url, data=None, headers=headers)
    with urllib.request.urlopen(req) as response:
      mp4data = response.read()
      mp4soup = BeautifulSoup(mp4data,features='html.parser')
    #with open('mp4data.txt','wb') as f:
    #  f.write(mp4data)
    vdata = [v.attrs for v in mp4soup.find_all('video')]
    vdata.sort(key=lambda y: int(y['system-bitrate']),reverse=True)
    #print(json.dumps(vdata,indent=2))
    vurl = None
    vwidth = None
    vheight = None
    for v in vdata:
      try:
        #print('Trying {}'.format(v['src']))
        req = urllib.request.Request(v['src'], data=None, headers=headers)
        vurl = urllib.request.urlopen(req).geturl()
        try:
          vwidth = int(v['width'])
          vheight = int(v['height'])
        except KeyError:
          pass
        #print('SUCCESS: {}'.format(vurl))
        break # If successful, then stop searching for the videos
      except urllib.error.HTTPError:
        #print('FAILED!')
        pass # Move on to the next highest-bitrate video
    x['data']['mp4'] = {}
    x['data']['mp4']['url'] = vurl
    x['data']['mp4']['filename'] = re.split(r'^(.+)\/([^\/]+.mp4)(.*)$',vurl)[2]
    x['data']['mp4']['width'] = vwidth
    x['data']['mp4']['height'] = vheight
  except TypeError:
    x['data']['mp4'] = None


####################
### MAIN PROGRAM ###
####################

# Get the raw list of input URLs from the command line and read any URL files provided.
try:
  url_list = []
  for u in args.url:
    if Path(u).is_file():
      try:
        with open(u) as f:
          v = f.read().splitlines()
        url_list.extend(v)
      except UnicodeDecodeError:
        url_list.extend([u])
    else:
      url_list.extend([u])
  if len(url_list)==0:
    url_list = None
except IndexError:
  url_list = None

# Parse the URLs and kick out any that could not be parsed.
if url_list == None:
  parser.print_help()
else:
  items = []
  for url in url_list:
    if url[0] != '#': # Ignore commented-out items
      i = parseUrl(url)
      if i != None:
        items.extend([i])
      else:
        print("{}ERROR: No rules to interpret `{}`.{}".format(bcolors.FAIL,url,bcolors.ENDC))

# Download the URLs
for item in items:
  print("{}Downloading `{}`...{}".format(bcolors.OKBLUE,item['url'],bcolors.ENDC))
  initializeDownload(item)
  getMP4Link(item)

# Debug output
print(json.dumps(items,indent=2))

retcd=0
sys.exit(retcd)
