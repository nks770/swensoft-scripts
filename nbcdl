#!/bin/env python3

import argparse
import json
import re
import subprocess
from pathlib import Path
from datetime import datetime, timezone, timedelta
#import pytz
import urllib.request
from bs4 import BeautifulSoup

user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'

# Define terminal colors
class bcolors:
  HEADER = '\033[95m'
  OKBLUE = '\033[94m'
  OKGREEN = '\033[92m'
  WARNING = '\033[93m'
  FAIL = '\033[91m'
  ENDC = '\033[0m'
  BOLD = '\033[1m'
  UNDERLINE = '\033[4m'

def import_m3u(m3ufile):
  m3uout = []
  item = {}
  for line in m3ufile:
    p1 = re.split(r'^#(.+):(.+)$',line)
    p2 = re.split(r'^#(.+):([\d\.]+),*$',line)
    p3 = re.split(r'^#(.+):(\d+),*$',line)
    if line in ('#EXTM3U' '#EXT-X-ENDLIST') or line.strip() == "":
      continue
    elif len(p3)==4:
      try:
        item[p3[1]] = int(p3[2])
      except:
        item[p3[1]] = p3[2]
    elif len(p2)==4:
      try:
        item[p2[1]] = float(p2[2])
      except:
        item[p2[1]] = p2[2]
    elif len(p1)==4: 
      ext3d = {}
      for p in re.findall(r'([^#:\,]+)=([^"\,]+|"[^"]*")',p1[2]):
        try:
          ext3d[p[0]]=int(p[1])
        except:
          ext3d[p[0]]=p[1]
      if len(ext3d)>0:
        item[p1[1]] = ext3d
      else:
        item[p1[1]] = p1[2]
    else:
      item['url']=line
      m3uout.extend([item])
      item = {}
  return m3uout

# Parse arguments
parser = argparse.ArgumentParser(description='Download/archive an NBC video stream.')
parser.add_argument('url',metavar='url',nargs=1,default='',
                    help='URL of video to download.')
parser.add_argument('-t','--test', action='store_true', dest='test',
                    help='Test mode, do not actually save any files.')
parser.add_argument('-f','--force', action='store_true', dest='force',
                    help='Overwrite data if already exists.')
args = parser.parse_args()
url = args.url[0]


# Verify provided URL
link = re.split(r'(?x)^(?:https)?:?\/\/((?:www\.)?(?:nbcnews)\.com)\/([^\/]+)\/([^\/]+)\/((.+)-(.+))$',url)
type = 'nbcnews'

if len(link)<8:
  link = re.split(r'(?x)^(?:https)?:?\/\/((?:www\.)?(?:nbc)\.com)\/([^\/]+)\/([^\/]+)\/((.+)\/(.+))$',url)
  type = 'nbc'

clean_url = 'https://{}/{}/{}/{}'.format(link[1],link[2],link[3],link[4])

if len(link)<8:
  raise Exception("Unhandled URL pattern.")


# Download webpage
headers = {'User-Agent': user_agent}
req = urllib.request.Request(clean_url, data=None, headers=headers)
with urllib.request.urlopen(req) as response:
  webpage = response.read()
  soup = BeautifulSoup(webpage,features='html.parser')

nextdata = soup.find(attrs={'id':'__NEXT_DATA__'}).string
jsondata = json.loads(nextdata)
videoAssets = jsondata['props']['initialState']['video']['current']['videoAssets']
videoAssets.sort(key=lambda x: x['bitrate'],reverse=True)
videoAsset = videoAssets[0]['publicUrl']

videoTitle = jsondata['props']['initialState']['video']['current']['headline']['primary']
videoDesc = jsondata['props']['initialState']['video']['current']['description']['primary']
dateBroadcast = jsondata['props']['initialState']['video']['current']['dateBroadcast']
guid = jsondata['props']['initialState']['video']['current']['mpxMetadata']['guid']


# This converts the UTC to Central Time in a rudimentary way
# It would be better to use pyzt, but this calculation doesnt need to be
# really precise.  I am OK with it being off by an hour or two.
broadcastDate = datetime.strptime(dateBroadcast, '%a %b %d %Y %H:%M:%S %Z%z (UTC)')
broadcastDateCT = broadcastDate.replace(tzinfo=timezone(timedelta(seconds=-21600), 'CT')) - timedelta(seconds=21600)


# Get URL for closed captions (SRT) 
try:
  closedCaptioning = jsondata['props']['initialState']['video']['current']['closedCaptioning']
  closedCaptions = urllib.request.urlopen(closedCaptioning['srt']).geturl()
except:
  closedCaptions = None
#print(closedCaptions)


# Resolve link (MP4)
headers = {'User-Agent': user_agent}
req = urllib.request.Request(videoAsset, data=None, headers=headers)
with urllib.request.urlopen(req) as response:
 vdata1 = response.read()
 soup1 = BeautifulSoup(vdata1,features='html.parser')
xdata1 = [x.attrs for x in soup1.find_all('video')]
xdata1.sort(key=lambda x: int(x['system-bitrate']),reverse=True)
ydata1 = xdata1[0]['src']
mp4link=urllib.request.urlopen(ydata1).geturl()
#print(mp4link)


# Resolve link (M3U8)
headers = {'User-Agent': user_agent}
req = urllib.request.Request('{}&manifest=m3u'.format(videoAsset), data=None, headers=headers)
with urllib.request.urlopen(req) as response:
 vdata2 = response.read()
 soup2 = BeautifulSoup(vdata2,features='html.parser')
xdata2 = [x.attrs for x in soup2.find_all('video')]
ydata2 = xdata2[0]['src']
master_m3ulink=urllib.request.urlopen(ydata2).geturl()
#print(master_m3ulink)


# Download master.m3u8
headers = {'User-Agent': user_agent}
req = urllib.request.Request(master_m3ulink, data=None, headers=headers)
with urllib.request.urlopen(req) as response:
  master_m3u8 = response.read().decode('utf-8').split('\n')
m3ulist = import_m3u(master_m3u8)
m3ulist.sort(key=lambda x: x['EXT-X-STREAM-INF']['BANDWIDTH'],reverse=True)
m3uitem = m3ulist[0]
#print(json.dumps(m3uitem,indent=2))


# Download index.m3u8
headers = {'User-Agent': user_agent}
req = urllib.request.Request(m3uitem['url'], data=None, headers=headers)
with urllib.request.urlopen(req) as response:
  index_m3u8 = response.read().decode('utf-8').split('\n')
m3uindex = import_m3u(index_m3u8)
#print(json.dumps(m3uindex,indent=2))

# Determine the date code for the program
pgm_date = broadcastDateCT.strftime('%y%m%d')
chk_date = re.split(r'(?:.*)(\d{6})(?:.*)',guid)
if len(chk_date)>2:
  if pgm_date != chk_date[1]:
    raise Exception('ERROR: Ambiguity in program date between [{}] and {}.'.format(broadcastDateCT,guid))


# Assemble final program data
pgmdata = {}
pgmdata['datecode']=pgm_date
pgmdata['name']=link[4]
pgmdata['guid']=guid
pgmdata['title']=videoTitle
pgmdata['desc']=videoDesc
pgmdata['webpage']=clean_url
pgmdata['asset_url']=videoAsset
pgmdata['captions_url']=closedCaptions
pgmdata['mp4_url']=mp4link
pgmdata['master_url']=master_m3ulink
pgmdata['m3u8']=m3uitem
pgmdata['m3u8']['playlist']=re.split(r'^(.+)\/([^\?]+)(.*)$',m3uitem['url'])[2]

dir='{}_{}'.format(pgm_date,link[4])

file_exists = False
if Path('.archive/{}.zip'.format(dir)).is_file() or Path('{}.mp4'.format(dir)).is_file():
  file_exists = True

commands = []
if not args.test:
  if file_exists:
    if args.force:
      if Path('.archive/{}.zip'.format(dir)).is_file():
        commands.extend([['rm','-fv','.archive/{}.zip'.format(dir)]])
      if Path('{}.mp4'.format(dir)).is_file():
        commands.extend([['rm','-fv','{}.mp4'.format(dir)]])
    else:
      print("Nothing to do.")
      quit()

# Create list of shell commands
copycmd = []
cleancmd = []
archive = ['zip','-9r','.archive/{}.zip'.format(dir),dir]
rmcmd = ['rm','-rf',dir]

if pgmdata['master_url'] != None:
  commands.extend([['wget',pgmdata['master_url'],'-O','{}/ts/master.m3u8'.format(dir)]])
  commands.extend([['wget',m3uitem['url'],'-O','{}/ts/{}'.format(dir,pgmdata['m3u8']['playlist'])]])
  for tsfile in m3uindex:
    commands.extend([['wget',tsfile['url'],'-O','{}/ts/{}'.format(dir,re.split(r'^(.+)\/([^\?]+)(.*)$',tsfile['url'])[2])]])
  copycmd = ['ffmpeg','-i','{}/{}.ts'.format(dir,guid),'-acodec','copy','-vcodec','copy','{}.mp4'.format(dir)]
  cleancmd = ['rm','-fv','{}/{}.ts'.format(dir,guid)]

if pgmdata['mp4_url'] != None:
  commands.extend([['wget',pgmdata['mp4_url'],'-O','{}/{}'.format(dir,re.split(r'^(.+)\/([^\?]+)(.*)$',pgmdata['mp4_url'])[2])]])
  copycmd = ['cp','-av','{}/{}'.format(dir,re.split(r'^(.+)\/([^\?]+)(.*)$',pgmdata['mp4_url'])[2]),'{}.mp4'.format(dir)]
  cleancmd = []

if pgmdata['captions_url'] != None:
  commands.extend([['wget',pgmdata['captions_url'],'-O','{}/{}'.format(dir,re.split(r'^(.+)\/([^\?]+)(.*)$',pgmdata['captions_url'])[2])]])

if len(copycmd)>0:
  commands.extend([copycmd])
if len(cleancmd)>0:
  commands.extend([cleancmd])

commands.extend([archive,rmcmd])

if args.test:
  print(json.dumps(pgmdata,indent=2))

  for cmd in commands:
    print('{}{}{}'.format(bcolors.OKGREEN,cmd,bcolors.ENDC))

else:
  # Create directories
  Path('.archive').mkdir(parents=True,exist_ok=True)
  Path('{}/response_data'.format(dir)).mkdir(parents=True,exist_ok=True)
  if pgmdata['master_url'] != None:
    Path('{}/ts'.format(dir)).mkdir(parents=True,exist_ok=True)

  # Write data
  print('Writing {}/pgmdata.json...'.format(dir))
  with open('{}/pgmdata.json'.format(dir),'w') as f:
    json.dump(pgmdata,f)
  print('Writing {}/response_data/webpage...'.format(dir))
  with open('{}/response_data/webpage'.format(dir),'wb') as f:
    f.write(webpage)
  print('Writing {}/response_data/asset_mp4...'.format(dir))
  with open('{}/response_data/asset_mp4'.format(dir),'wb') as f:
    f.write(vdata1)
  print('Writing {}/response_data/asset_m3u8...'.format(dir))
  with open('{}/response_data/asset_m3u8'.format(dir),'wb') as f:
    f.write(vdata2)

  # Combine TS files, if required
  if copycmd[0] == 'ffmpeg':
    with open('{}/{}.ts'.format(guid),'wb') as out:
      for ts['url'] in m3uindex:
        with open('{}/ts/{}'.format(dir,re.split(r'^(.+)\/([^\?]+)(.*)$',tsfile['url'])[2]),'rb') as inp:
          out.write(inp.read())

  # Run commands
  for cmd in commands:
    #print('{}{}{}'.format(bcolors.OKGREEN,cmd,bcolors.ENDC))
    subprocess.run(cmd,capture_output=False,check=True)
