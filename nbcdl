#!/bin/env python3

import sys
import argparse
import copy
import json
import demjson
import re
import subprocess
from pathlib import Path
from datetime import datetime, timezone, timedelta
#import pytz
import urllib.request, urllib.parse
from bs4 import BeautifulSoup

#user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'
firefox_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko/20100101 Firefox/84.0'
firefox_headers = {'User-Agent': firefox_agent}

user_agent = 'Lavf/58.45.100'
headers = {'User-Agent': user_agent}

stats = {'attempted':0,'success':0,'skipped':0}
ambiguity = []

# Define terminal colors
class bcolors:
  HEADER = '\033[95m'
  OKBLUE = '\033[94m'
  OKGREEN = '\033[92m'
  WARNING = '\033[93m'
  FAIL = '\033[91m'
  ENDC = '\033[0m'
  BOLD = '\033[1m'
  UNDERLINE = '\033[4m'

def import_m3u(m3ufile):
  m3uout = []
  item = {}
  for line in m3ufile.decode('utf-8').split('\n'):
    p1 = re.split(r'^#(.+):(.+)$',line)
    p2 = re.split(r'^#(.+):([\d\.]+),*$',line)
    p3 = re.split(r'^#(.+):(\d+),*$',line)
    p4 = re.split(r'^\#EXT-X-KEY:([^\=]+)=([^\,]+),([^\=]+)=\"?([^\"]+)\"?',line)
    if line in ('#EXTM3U' '#EXT-X-ENDLIST') or line.strip() == "":
      continue
    elif line[:10] == "#EXT-X-KEY":
      item = {'EXT-X-KEY':{p4[1]:p4[2],p4[3]:p4[4]}}
    elif len(p3)==4:
      try:
        item[p3[1]] = int(p3[2])
      except:
        item[p3[1]] = p3[2]
    elif len(p2)==4:
      try:
        item[p2[1]] = float(p2[2])
      except:
        item[p2[1]] = p2[2]
    elif len(p1)==4: 
      ext3d = {}
      for p in re.findall(r'([^#:\,]+)=([^"\,]+|"[^"]*")',p1[2]):
        try:
          ext3d[p[0]]=int(p[1])
        except:
          ext3d[p[0]]=p[1]
      if len(ext3d)>0:
        item[p1[1]] = ext3d
      else:
        item[p1[1]] = p1[2]
    else:
      item['url']=line
      m3uout.extend([item])
      item = {}
  return m3uout


def clean_bracket(s):
  b = 0
  for i in range(len(s)):
    if s[i]=='{':
      b=b+1
    if s[i]=='}':
      b=b-1
    if b==0:
      break
  return s[:i+1]

def quarter(mm):
  if mm in ('01','02','03'):
    return 'Q1'
  elif mm in ('04','05','06'):
    return 'Q2'
  elif mm in ('07','08','09'):
    return 'Q3'
  elif mm in ('10','11','12'):
    return 'Q4'
  else:
    return None

def parseUrl(raw_url):
  try:
    ld = re.split(r'(?x)^(?:http)?(?:s)?:?\/\/((?:www\.)?(?:nbcnews)\.com)\/([^\/]+)\/([^\/]+)\/((.+)-(.+))$',raw_url)
    return {'url':'https://{}/{}/{}/{}'.format(ld[1],ld[2],ld[3],ld[4]),
            'server':ld[1],
            'name':ld[5],
            'id':ld[6]}
  except IndexError:
    try:
      ld = re.split(r'(?x)^(?:http)?(?:s)?:?\/\/((?:www\.)?(?:nbcnews)\.com)\/([^\/]+)\/((.+)-(.+))$',raw_url)
      return {'url':'https://{}/{}/{}'.format(ld[1],ld[2],ld[3]),
              'server':ld[1],
              'name':ld[4],
              'id':ld[5]}
    except IndexError:
      try:
        ld = re.split(r'(?x)^(?:http)?(?:s)?:?\/\/((?:www\.)?(?:nbc)\.com)\/([^\/]+)\/([^\/]+)\/((.+)\/(.+))$',raw_url)
        return {'url':'https://{}/{}/{}/{}'.format(ld[1],ld[2],ld[3],ld[4]),
                'server':ld[1],
                'name':ld[5],
                'id':ld[6]}
      except IndexError:
        return None


# Parse arguments
parser = argparse.ArgumentParser(description='Download/archive an NBC video stream.')
parser.add_argument('url',metavar='url',nargs='*',default='',
                    help='URL of video to download.')
parser.add_argument('-c','--captions', action='store_true', dest='captions',
                    help='When processing/saving a video, also output the captions to a folder.')
parser.add_argument('-d','--dryrun', action='store_true', dest='dryrun',
                    help='Dry run mode, generate commands but do not actually run them.')
parser.add_argument('-f','--force', action='store_true', dest='force',
                    help='Overwrite data folder and output MP4 if already exists.')
parser.add_argument('-ff','--force-archive', action='store_true', dest='forcearc',
                    help='Works like "-f" but also purges and overwrites archive files.')
parser.add_argument('-m','--force-m3u', action='store_true', dest='force_m3u',
                    help='Force code to transcode M3U even if MP4 is available.')
parser.add_argument('-n','--nodelete', action='store_true', dest='no_delete',
                    help='Do not remove temporary files.')
parser.add_argument('-q','--quick', action='store_true', dest='qcheck',
                    help='Quick check mode quickly checks if the MP4 was already downloaded without going online.')
parser.add_argument('-r','--restart', metavar='DIRECTORY', dest='prevdir',default='',
                    help='Restart mode, skip the download and only do the finalization steps.')
#parser.add_argument('-s','--search', metavar=('WEB_URL','OUTPUT_DIR'),dest='search',
#                    nargs=2,default=['',''],
#                    help='Search for URLs on website, and make URL download files.')
parser.add_argument('-v','--verify', metavar='DIRECTORY', dest='verify',default='',
                    help='Verify mode, download all the URLs again and check for differences.')
parser.add_argument('-e','--secure', action='store_true', dest='securemode',
                    help='Secure download mode; check file integrity by downloading multiple times.')
parser.add_argument('-p','--permissive', action='store_true', dest='permissive',
                    help='Permissive mode; do not produce an error if no video data is available.')
parser.add_argument('-s','--search', metavar='WEB_SITEURL', dest='search',default='',
                    help='Search a given website for video URLs, and make a list.')
parser.add_argument('-o','--output', metavar='OUTPUT_DIR', dest='searchout',default='',
                    help='Only applies when used with "-s"; output found URLs to "url.txt" files.')
parser.add_argument('-Q','--quarter', action='store_true', dest='by_quarter',
                    help='When used with "-s", organize files by quarter instead of by month.')
parser.add_argument('-t','--test', action='store_true', dest='test',
                    help='Test mode, only test the URL and display the JSON.')
parser.add_argument('-N','--nocolor', action='store_true', dest='nocolor',
                    help='Do not use terminal colors.')
parser.add_argument('-S','--skip-older-than', metavar='HOURS', dest='skiptime',default='',
                    help='Skip files older than a specified amount of time.')
args = parser.parse_args()

# Remove colors from terminal
if args.nocolor:
  bcolors.HEADER = ''
  bcolors.OKBLUE = ''
  bcolors.OKGREEN = ''
  bcolors.WARNING = ''
  bcolors.FAIL = ''
  bcolors.ENDC = ''
  bcolors.BOLD = ''
  bcolors.UNDERLINE = ''


def CreateDownload(url):

  stats['attempted'] = stats['attempted'] + 1

  # Verify provided URL
  link = parseUrl(url)
  if link==None:
    return None
  
  if args.qcheck:
    qcheck = list(Path('.').glob('*{}*mp4'.format(link['name']))) 
    qcheck.extend(list(Path('.archive').glob('*{}*zip'.format(link['name']))))
    if len(qcheck)>0:
      print('{}Found that {} already exits, nothing to do.{}'.format(bcolors.OKGREEN,str(qcheck[0]),bcolors.ENDC))
      stats['skipped'] = stats['skipped'] + 1
      return None

  # Download webpage
  req = urllib.request.Request(link['url'], data=None, headers=headers)
  with urllib.request.urlopen(req) as response:
    webpage = response.read()
    soup = BeautifulSoup(webpage,features='html.parser')
  
  # Initialize program data dictionary
  pgmdata = {}
  pgmdata['webpage']=link['url']
  
  if 'nbcnews.com' in link['server']:
    # Get NEXT_DATA (JSON)
    nextdata = soup.find(attrs={'id':'__NEXT_DATA__'}).string
    jsondata = json.loads(nextdata)
    videoAssets = jsondata['props']['initialState']['video']['current']['videoAssets']
    videoAssets.sort(key=lambda x: x['bitrate'],reverse=True)
    
    dateBroadcast = jsondata['props']['initialState']['video']['current']['dateBroadcast']
  
    # This converts the UTC to Central Time in a rudimentary way
    # It would be better to use pyzt, but this calculation doesnt need to be
    # really precise.  I am OK with it being off by an hour or two.
    broadcastDate = datetime.strptime(dateBroadcast, '%a %b %d %Y %H:%M:%S %Z%z (UTC)')
    broadcastDateCT = broadcastDate.replace(tzinfo=timezone(timedelta(seconds=-21600), 'CT')) - timedelta(seconds=21600)
  
    # Determine the date code for the program
    pgmdata['datecode'] = broadcastDateCT.strftime('%y%m%d')
  
    pgmdata['name'] = link['name']
    pgmdata['guid'] = jsondata['props']['initialState']['video']['current']['mpxMetadata']['guid']
    pgmdata['title'] = jsondata['props']['initialState']['video']['current']['headline']['primary']
    pgmdata['desc'] = jsondata['props']['initialState']['video']['current']['description']['primary']
    try:
      pgmdata['thumbnail'] = {}
      pgmdata['thumbnail']['url'] = jsondata['props']['initialState']['video']['current']['primaryImage']['url']['primary']
      pgmdata['thumbnail']['filename'] = re.split(r'^([^\?]+)\/([^\?]+)(.*)$',pgmdata['thumbnail']['url'])[2]
    except (KeyError, IndexError) as e:
      pgmdata['thumbnail'] = None
  
    # Get URL for closed captions (SRT) 
    try:
      pgmdata['captions'] = {}
      closedCaptioning = jsondata['props']['initialState']['video']['current']['closedCaptioning']
      pgmdata['captions']['url'] = urllib.request.urlopen(closedCaptioning['srt']).geturl()
      pgmdata['captions']['filename'] = re.split(r'^([^\?]+)\/([^\?]+)(.*)$',pgmdata['captions']['url'])[2]
    except (KeyError, IndexError, TypeError) as e:
      pgmdata['captions'] = None
  
    # Video asset URLs
    #print(json.dumps(pgmdata,indent=2,default=str))
    #print(json.dumps(videoAssets,indent=2,default=str))
    if len(videoAssets)==0:
      pgmdata['asset'] = None
    else:
      assetUrl = re.split(r'^([^\?]+)\?(.+)$',videoAssets[0]['publicUrl'])
      if len(assetUrl)>2:
        pgmdata['asset'] = {'base':assetUrl[1],
                      'ext': assetUrl[2]}
    #                'mp4': '',
    #                'm3u': 'manifest=m3u'}
      else:
        pgmdata['asset'] = {'base':videoAssets[0]['publicUrl'],
                      'ext': None}
    #                'mp4': '',
    #                'm3u': 'manifest=m3u'}
  
    # Check the date code
    chk_date = re.split(r'(?:.*)(\d{6})(?:.*)',pgmdata['guid'])
    if len(chk_date)>2:
      if pgmdata['datecode'] != chk_date[1]:
        #raise Exception('ERROR: Ambiguity in program date between [{}] and [{}].'.format(broadcastDateCT,pgmdata['guid']))
        ambiguity.extend([{'url':link['url'],'airdate':broadcastDateCT,'guid':pgmdata['guid'],'final_date':chk_date[1]}])
        pgmdata['datecode'] = chk_date[1]
  
  #  print(json.dumps(jsondata,indent=2))
  
  if 'nbc.com' in link['server']:
  
    # Get LD+JSON (JSON)
    ldjson = json.loads(soup.find('script',attrs={'type':re.compile('json')}).string)
  
    # Get EXPORTS (JSON)
    exports = webpage.decode('utf-8').split('.exports=')
    for i in reversed(range(len(exports))):
      exports[i] = clean_bracket(exports[i])
      if exports[i][0] != '{' or 'MPX' not in exports[i]:
        exports.pop(i)
    expdata = re.sub(r'\!([01{1}])',r'\1',exports[0])
    jsondata = demjson.decode(expdata)
  
    # Get PRELOAD (JSON)
    preload = json.loads(re.split(r'^PRELOAD\=(.+)$',soup.find('script',text=re.compile(r'^PRELOAD\=(.+)$')).string)[1])
    preload_pages=preload['pages'][list(preload['pages'].keys())[0]]
  
    # Get data from player link
    plink = '{}/p/{}/{}/select/media/guid/{}/{}'.format(jsondata['MPX']['domain'],jsondata['MPX']['pid'],jsondata['MPX']['playerName'],jsondata['MPX']['id'],link['id'])
  #  print(plink)
    req = urllib.request.Request(plink, data=None, headers=headers)
    with urllib.request.urlopen(req) as response:
      pdata = response.read()
      psoup = BeautifulSoup(pdata,features='html.parser')
    #print(pdata.decode('utf-8'))
   
    # Determine videoAsset link
    try:
      plink2a = psoup.find('link',attrs={'rel':'alternate','type':re.compile('smil')})['href']
    except TypeError:
      plink2a = None
    try:
      plink2b = re.findall(r'(?i:tp\:releaseurl)=\"([^\"]+)\"',pdata.decode('utf-8'))[0]
    except IndexError:
      plink2b = None
    try:
      d = psoup.find('link',attrs={'rel':'alternate','type':re.compile('rss\+xml')})['href']
      e = re.split(r'(.*)linkUrl=(.+)\%2Fselect',d)[2].replace('%3A',':').replace('%2F','/')
      f = re.split(r'(?:.*)player.theplatform.com\/p\/([^\/]+)\/([^\/]+)',e)
      plink2c = 'https://link.theplatform.com/s/{}/media/{}?player=%5Bv2%5D%20OneApp%20-%20PDK6%20NBC.com'.format(f[1],f[2])
    except (IndexError,TypeError) as e:
      plink2c = None
      
    if plink2a == None:
      plink2a = plink2b
    if plink2a == None:
      plink2a = plink2c
    if plink2a != None and plink2b != None and plink2a != plink2b:
      raise Exception("ERROR: Links different.\nlink1={}\nlink2={}.".format(plink2a,plink2b))

    previewUrl = '{}&format=preview&formats=MPEG-DASH+widevine,M3U+appleHlsEncryption,M3U+none,MPEG-DASH+none,MPEG4,MP3'.format(plink2a)
    try:
      req = urllib.request.Request(previewUrl, data=None, headers=headers)
      with urllib.request.urlopen(req) as response:
        preview = response.read()
      preview_json = json.loads(preview.decode('utf-8'))
    except urllib.error.HTTPError:
      return None
  
  #  print(psoup.decode('utf-8'))
  #  print(json.dumps(ldjson,indent=2))
  #  print(json.dumps(jsondata,indent=2))
  #  print(json.dumps(preload_pages,indent=2))
  #  print(json.dumps(preview_json,indent=2))
  
    # Broadcast date
    try:
      dateBroadcast = preload_pages['metadata']['airDate']
    except KeyError:
      dateBroadcast = ldjson['video']['uploadDate']
  
    # This converts the UTC to Central Time in a rudimentary way
    # It would be better to use pyzt, but this calculation doesnt need to be
    # really precise.  I am OK with it being off by an hour or two.
    broadcastDate = datetime.strptime(dateBroadcast, '%Y-%m-%dT%H:%M:%S.%fZ')
    broadcastDateCT = broadcastDate.replace(tzinfo=timezone(timedelta(seconds=-21600), 'CT')) - timedelta(seconds=21600)
  
    # Determine the date code for the program
    pgmdata['datecode'] = broadcastDateCT.strftime('%y%m%d')
    #chk_date = re.split(r'(?:.*)(\d{6})(?:.*)',guid)
    #if len(chk_date)>2:
    #  if datecode != chk_date[1]:
    #    raise Exception('ERROR: Ambiguity in program date between [{}] and {}.'.format(broadcastDateCT,guid))
    pgmdata['name'] = link['name']
    pgmdata['guid'] = link['id']
  
    # Video title
    try:
      pgmdata['title'] = ldjson['video']['name']
    except KeyError:
      try:
        pgmdata['title'] = preload_pages['metadata']['secondaryTitle']
      except KeyError:
        pgmdata['title'] = psoup.find('meta',attrs={'property':'og:title'})['content']
  
    # Video description
    try:
      pgmdata['desc'] = preload_pages['metadata']['ariaLabel']
    except KeyError:
      try:
        pgmdata['desc'] = preload_pages['metadata']['title']
      except KeyError:
        try:
          pgmdata['desc'] = preload_pages['metadata']['description']
        except KeyError:
          try:
            pgmdata['desc'] = ldjson['video']['description']
          except KeyError:
            pgmdata['desc'] = psoup.find('meta',attrs={'property':'og:description'})['content']
  
    # Webpage URLs
    pgmdata['webpage'] = link['url']
  
    # Thumbnail image URL
    try:
      pgmdata['thumbnail'] = {}
      try:
        pgmdata['thumbnail']['url'] = ldjson['video']['thumbnailUrl']
      except KeyError:
        try:
          pgmdata['thumbnail']['url'] = preload_pages['metadata']['image']
        except KeyError:
          pgmdata['thumbnail']['url'] = psoup.find('meta',attrs={'property':'og:image'})['content']
      pgmdata['thumbnail']['filename'] = re.split(r'^([^\?]+)\/([^\?]+)(.*)$',pgmdata['thumbnail']['url'])[2]
    except (KeyError, IndexError) as e:
      pgmdata['thumbnail'] = None
  
    # Get the closed captions file
    try:
      pgmdata['captions'] = {}
      for cap in preview_json['captions']:
        if cap['lang'] == 'en':
          pgmdata['captions']['url'] = cap['src']
      pgmdata['captions']['filename'] = re.split(r'^([^\?]+)\/([^\?]+)(.*)$',pgmdata['captions']['url'])[2]
    except (KeyError, IndexError) as e:
      pgmdata['captions'] = None
  
    # Video asset URLs
    assetUrl = re.split(r'^([^\?]+)\?(.+)$',plink2a)
    if len(assetUrl)>2:
      pgmdata['asset'] = {'base':assetUrl[1],
                    'ext': assetUrl[2]}
    else:
      pgmdata['asset'] = {'base':plink2a,
                    'ext': None}
  
    
  pgmdata['data'] = {}
  pgmdata['data']['mp4'] = {}
  pgmdata['data']['m3u'] = {}
  
  # Get MP4 file link
  #url = '{}?mbr=true'.format(pgmdata['asset']['base'])
  try:
    url = '{}?mbr=true&player=%5Bv2%5D%20OneApp%20-%20PDK6%20NBC.com&format=SMIL&formats=MPEG4'.format(pgmdata['asset']['base'])
    req = urllib.request.Request(url, data=None, headers=headers)
    with urllib.request.urlopen(req) as response:
     vdata1 = response.read()
     soup1 = BeautifulSoup(vdata1,features='html.parser')
    #print(vdata1.decode('utf-8'))
    xdata1 = [x.attrs for x in soup1.find_all('video')]
    xdata1.sort(key=lambda x: int(x['system-bitrate']),reverse=True)
  except TypeError:
    xdata1 = None
  #print(json.dumps(xdata1,indent=2))
  try:
    pgmdata['data']['mp4']['url'] = urllib.request.urlopen(xdata1[0]['src']).geturl()
    pgmdata['data']['mp4']['filename'] = re.split(r'^(.+)\/([^\/]+.mp4)(.*)$',pgmdata['data']['mp4']['url'])[2]
    try:
      pgmdata['data']['mp4']['width'] = int(xdata1[0]['width'])
      pgmdata['data']['mp4']['height'] = int(xdata1[0]['height'])
    except KeyError:
      pgmdata['data']['mp4']['width'] = None
      pgmdata['data']['mp4']['height'] = None
  except (urllib.error.HTTPError, IndexError, TypeError) as e:
  #  print(xdata1[0]['src'])
    pgmdata['data']['mp4'] = None
  
  
  # Get M3U file link
  try:
    #url = '{}?mbr=true&manifest=m3u'.format(pgmdata['asset']['base'])
    url = '{}?mbr=true&format=SMIL&manifest=m3u'.format(pgmdata['asset']['base'])
    #url = '{}?player=%5Bv2%5D%20OneApp%20-%20PDK6%20NBC.com&format=SMIL&formats=MPEG-DASH+widevine,M3U+appleHlsEncryption,M3U+none,MPEG-DASH+none,MPEG4,MP3'.format(pgmdata['asset']['base'])
    #url = '{}?player=%5Bv2%5D%20OneApp%20-%20PDK6%20NBC.com&manifest=m3u&format=SMIL&formats=MPEG-DASH+widevine,M3U+appleHlsEncryption,M3U+none,MPEG-DASH+none,MPEG4,MP3'.format(pgmdata['asset']['base'])
    req = urllib.request.Request(url, data=None, headers=headers)
    with urllib.request.urlopen(req) as response:
     vdata2 = response.read()
     soup2 = BeautifulSoup(vdata2,features='html.parser')
    #print(vdata2.decode('utf-8'))
    xdata2 = [x.attrs for x in soup2.find_all('video')]
    xdata2.sort(key=lambda x: int(x['width']),reverse=True)
  except TypeError:
    xdata2 = None
  #print(json.dumps(xdata2[0],indent=2))
  try:
    if len(xdata2[0]['abstract']) > len(pgmdata['desc']):
      pgmdata['desc'] = xdata2[0]['abstract']
  except (IndexError,TypeError) as e:
    pass
  try:
    pgmdata['data']['m3u']['master'] = urllib.request.urlopen(xdata2[0]['src']).geturl()
  
    # Download master.m3u8
    #print(pgmdata['data']['m3u']['master'])
    req = urllib.request.Request(pgmdata['data']['m3u']['master'], data=None, headers=headers)
    with urllib.request.urlopen(req) as response:
      vdata3 = response.read()
    #print(vdata3.decode('utf-8'))
    xdata3 = import_m3u(vdata3)
    xdata3.sort(key=lambda x: x['EXT-X-STREAM-INF']['BANDWIDTH'],reverse=True)
    #print(json.dumps(xdata3,indent=2))
    pgmdata['data']['m3u']['index'] = xdata3[0]['url']
    pgmdata['data']['m3u']['indexfile'] = re.split(r'^([^\?]+)\/([^\?]+)(.*)$',pgmdata['data']['m3u']['index'])[2]
    #print(json.dumps(xdata3[0],indent=2))
    try:
      resolution=re.split(r'(\d+)x(\d+)',xdata3[0]['EXT-X-STREAM-INF']['RESOLUTION'])
      pgmdata['data']['m3u']['width'] = int(resolution[1])
      pgmdata['data']['m3u']['height'] = int(resolution[2])
    except KeyError:
      pgmdata['data']['m3u']['width'] = None
      pgmdata['data']['m3u']['height'] = None
    
    
    # Open index.m3u8
    req = urllib.request.Request(pgmdata['data']['m3u']['index'], data=None, headers=headers)
    with urllib.request.urlopen(req) as response:
      vdata4 = response.read()
    #print(vdata4.decode('utf-8'))
    xdata4 = import_m3u(vdata4)
    try:
      pgmdata['data']['m3u']['encryption'] = {}
      pgmdata['data']['m3u']['encryption']['method'] = xdata4[0]['EXT-X-KEY']['METHOD']
      pgmdata['data']['m3u']['encryption']['keyurl'] = xdata4[0]['EXT-X-KEY']['URI']
      pgmdata['data']['m3u']['encryption']['keyfile'] = re.split(r'^([^\?]+)\/([^\?]+)(.*)$',pgmdata['data']['m3u']['encryption']['keyurl'])[2]
    except KeyError:
      pgmdata['data']['m3u']['encryption'] = None
    for i in range(1,len(xdata4)):
      if 'EXT-X-KEY' in xdata4[i].keys():
        raise Exception("Possibly multiple EXT-X-KEY groups detected.")
    
    # Get the M3U encryption key
    if pgmdata['data']['m3u']['encryption'] != None:
      req = urllib.request.Request(pgmdata['data']['m3u']['encryption']['keyurl'], data=None, headers=headers)
      with urllib.request.urlopen(req) as response:
        cryptokey = response.read()
      pgmdata['data']['m3u']['encryption']['key']=(''.join('{:02x}'.format(x) for x in cryptokey))
  
    pgmdata['data']['m3u']['playlist'] = []
    for ts in xdata4:
      tt = {}
      #tt['url'] = urllib.request.urlopen(ts['url']).geturl()
      tt['url'] = ts['url']
      tt['file'] = re.split(r'^([^\?]+)\/([^\?]+)(.*)$',tt['url'])[2]
      pgmdata['data']['m3u']['playlist'].extend([tt])
    
  except (urllib.error.HTTPError,IndexError,TypeError) as e:
    pgmdata['data']['m3u'] = None
    
  #print(json.dumps(xdata4,indent=2))
  if args.test or args.dryrun:
    pgmdata_ = copy.deepcopy(pgmdata)
    if pgmdata_['data']['m3u'] != None:
      pgmdata_['data']['m3u']['playlist'] = None
    print(json.dumps(pgmdata_,indent=2))
  if args.test:
    stats['skipped'] = stats['skipped'] + 1
    return None

  dir='{}_{}'.format(pgmdata['datecode'],pgmdata['name'])
  
  file_exists = False
  if Path('.archive/{}.zip'.format(dir)).is_file() or Path('{}.mp4'.format(dir)).is_file():
    file_exists = True
  
  if file_exists and not (args.force or args.forcearc):
    print("{}Files already downloaded. Nothing to do.{}".format(bcolors.OKGREEN,bcolors.ENDC))
    stats['skipped'] = stats['skipped'] + 1
    return None
  
  if Path('{}/metadata.json'.format(dir)).is_file() and not (args.force or args.forcearc):
    #with open('{}/metadata.json'.format(dir),'r') as j:
    #  pgmdata = json.load(j)
    print('{}Temporary directory already exists, please specify "-f" to overwrite.{}'.format(bcolors.WARNING,bcolors.ENDC))
    #return pgmdata
    return None

  if pgmdata['data']['mp4'] == None and pgmdata['data']['m3u'] == None and not args.permissive:
    print('{}No video data available for {}{}'.format(bcolors.FAIL,pgmdata['name'],bcolors.ENDC))
    return None
  
  commands = []
  
  if pgmdata['data']['m3u'] != None:
    #commands.extend([['wget','--user-agent={}'.format(user_agent),pgmdata['data']['m3u']['master'],'-O','{}/ts/master.m3u8'.format(dir)]])
    #commands.extend([['wget','--user-agent={}'.format(user_agent),pgmdata['data']['m3u']['index'],'-O','{}/ts/{}'.format(dir,pgmdata['data']['m3u']['indexfile'])]])
    for ts in pgmdata['data']['m3u']['playlist']:
      commands.extend([['wget','--user-agent={}'.format(user_agent),ts['url'],'-O','{}/ts/{}'.format(dir,ts['file'])]])
  
  if pgmdata['data']['mp4'] != None:
    commands.extend([['wget','--user-agent={}'.format(user_agent),pgmdata['data']['mp4']['url'],'-O','{}/{}'.format(dir,pgmdata['data']['mp4']['filename'])]])
  
  if pgmdata['captions'] != None:
    commands.extend([['wget',pgmdata['captions']['url'],'-O','{}/{}'.format(dir,pgmdata['captions']['filename'])]])

  if pgmdata['thumbnail'] != None:
    commands.extend([['wget',pgmdata['thumbnail']['url'],'-O','{}/{}'.format(dir,pgmdata['thumbnail']['filename'])]])
  
  if args.dryrun:
    for cmd in commands:
      print('{}{}{}'.format(bcolors.OKGREEN,cmd,bcolors.ENDC))
    stats['success'] = stats['success'] + 1
    return None
  
  else:
    # Create directories
    Path('{}/response_data'.format(dir)).mkdir(parents=True,exist_ok=True)
    if pgmdata['data']['m3u'] != None:
      Path('{}/ts'.format(dir)).mkdir(parents=True,exist_ok=True)
  
    # Write data
    print('Writing {}/metadata.json ...'.format(dir))
    with open('{}/metadata.json'.format(dir),'w') as f:
      json.dump(pgmdata,f,indent=2)
    print('Writing {}/response_data/webpage ...'.format(dir))
    with open('{}/response_data/webpage'.format(dir),'wb') as f:
      f.write(webpage)
    try:
      if pdata != None:
        print('Writing {}/response_data/player ...'.format(dir))
        with open('{}/response_data/player'.format(dir),'wb') as f:
          f.write(pdata)
    except UnboundLocalError:
      pass
    try:
      if preview != None:
        print('Writing {}/response_data/preview ...'.format(dir))
        with open('{}/response_data/preview'.format(dir),'wb') as f:
          f.write(preview)
    except UnboundLocalError:
      pass
    print('Writing {}/response_data/asset_mp4 ...'.format(dir))
    with open('{}/response_data/asset_mp4'.format(dir),'wb') as f:
      f.write(vdata1)
    print('Writing {}/response_data/asset_m3u8 ...'.format(dir))
    with open('{}/response_data/asset_m3u8'.format(dir),'wb') as f:
      f.write(vdata2)
    if pgmdata['data']['m3u'] != None:
      print('Writing {}/ts/master.m3u8 ...'.format(dir))
      with open('{}/ts/master.m3u8'.format(dir),'wb') as f:
        f.write(vdata3)
      print('Writing {}/ts/{} ...'.format(dir,pgmdata['data']['m3u']['indexfile']))
      with open('{}/ts/{}'.format(dir,pgmdata['data']['m3u']['indexfile']),'wb') as f:
        f.write(vdata4)
      if pgmdata['data']['m3u']['encryption'] != None:
        print('Writing {}/ts/{} ...'.format(dir,pgmdata['data']['m3u']['encryption']['keyfile']))
        with open('{}/ts/{}'.format(dir,pgmdata['data']['m3u']['encryption']['keyfile']),'wb') as f:
          f.write(cryptokey)
  
    # Run commands
    for cmd in commands:
      #print('{}{}{}'.format(bcolors.OKGREEN,cmd,bcolors.ENDC))
      try:
        subprocess.run(cmd,capture_output=False,check=True)
      except subprocess.CalledProcessError:
        try:
          # Try a second time
          subprocess.run(cmd,capture_output=False,check=True)
        except subprocess.CalledProcessError:
          # Try one last time
          subprocess.run(cmd,capture_output=False,check=True)

    return pgmdata

def ProcessData(pgmdata):

  if pgmdata == None:
    return None

  # Get the directory from the pgmdata
  dir='{}_{}'.format(pgmdata['datecode'],pgmdata['name'])

  # Exit subroutine if there is no video data available
  if pgmdata['data']['mp4'] == None and pgmdata['data']['m3u'] == None:
    if args.permissive:
      print("{}No mp4 or m3u data found. Proceeding.{}".format(bcolors.OKGREEN,bcolors.ENDC))
    else:
      print("{}No mp4 or m3u data found. Nothing to do.{}".format(bcolors.OKGREEN,bcolors.ENDC))
      return None

  if not Path('{}.mp4'.format(dir)).is_file() or (args.force or args.forcearc):

    if args.force or args.forcearc:
      Path('{}.mp4'.format(dir)).unlink(missing_ok=True)

    if (not args.force_m3u and pgmdata['data']['mp4'] != None) or (args.force_m3u and pgmdata['data']['m3u'] == None):
      copy = ['cp','-av','{}/{}'.format(dir,pgmdata['data']['mp4']['filename']),'{}.mp4'.format(dir)]
      print('{}{}{}'.format(bcolors.OKGREEN,copy,bcolors.ENDC))
      subprocess.run(copy,capture_output=False,check=True)
  
    elif pgmdata['data']['m3u'] != None:
      if pgmdata['data']['m3u']['encryption'] != None:
        # Construct commands to decrypt an encrypted HLS stream
        Path('{}/.crypto'.format(dir)).mkdir(parents=True,exist_ok=True)
        decryptcmd = []
        for ts in pgmdata['data']['m3u']['playlist']:
          with open('{}/.crypto/{}.enc'.format(dir,ts['file']),'wb') as enc:
            with open('{}/ts/{}'.format(dir,ts['file']),'rb') as src:
              s = src.read()
              # Drop the first 16 bytes from every ts file
              enc.write(s[16:])
              # But save the first 16 bytes, which is the IV needed for decryption
              ts['iv']=(''.join('{:02x}'.format(x) for x in s[:16]))
          decryptcmd.extend([['openssl','aes-128-cbc','-d','-in','{}/.crypto/{}.enc'.format(dir,ts['file']),'-out','{}/.crypto/{}'.format(dir,ts['file']),'-nosalt','-K',pgmdata['data']['m3u']['encryption']['key'].replace(' ',''),'-iv',ts['iv']]])
      
        # Run the decrypt commands
        for cmd in decryptcmd:
          print('{}{}{}'.format(bcolors.OKGREEN,cmd,bcolors.ENDC))
          subprocess.run(cmd,capture_output=False,check=True)
        
        # Combine decrypted files
        with open('{}/{}.ts'.format(dir,pgmdata['guid']),'wb') as out:
          for ts in pgmdata['data']['m3u']['playlist']:
            with open('{}/.crypto/{}'.format(dir,ts['file']),'rb') as inp:
              out.write(inp.read())
        subprocess.run(['rm','-rf','{}/.crypto'.format(dir)],capture_output=False,check=True)
  
      else:
        # Combine raw ts files that are not encrypted
        with open('{}/{}.ts'.format(dir,pgmdata['guid']),'wb') as out:
          for ts in pgmdata['data']['m3u']['playlist']:
            with open('{}/ts/{}'.format(dir,ts['file']),'rb') as inp:
              out.write(inp.read())
  
      ffmpeg = ['ffmpeg','-i','{}/{}.ts'.format(dir,pgmdata['guid']),'-acodec','copy','-vcodec','copy','{}.mp4'.format(dir)]
      subprocess.run(ffmpeg,capture_output=False,check=True)
      Path('{}/{}.ts'.format(dir,pgmdata['guid'])).unlink()

  # Copy captions to folder if requested
  if args.captions and pgmdata['captions'] != None:
    Path('captions').mkdir(parents=True,exist_ok=True)
    ext = re.split(r'(.+)\.(.+)',pgmdata['captions']['filename'])[2]
    copy = ['cp','-av','{}/{}'.format(dir,pgmdata['captions']['filename']),'captions/{}.{}'.format(dir,ext)]
    print('{}{}{}'.format(bcolors.OKGREEN,copy,bcolors.ENDC))
    subprocess.run(copy,capture_output=False,check=True)

  # Archive and remove data
  if not Path('.archive/{}.zip'.format(dir)).is_file() or args.forcearc:
    if args.forcearc:
      Path('.archive/{}.zip'.format(dir)).unlink(missing_ok=True)

    archive = ['zip','-9r','.archive/{}.zip'.format(dir),dir]
    Path('.archive').mkdir(parents=True,exist_ok=True)
    subprocess.run(archive,capture_output=False,check=True)
    if not args.no_delete:
      subprocess.run(['rm','-rf',dir],capture_output=False,check=True)

  stats['success'] = stats['success'] + 1


def verifyFile(url,fname):
  wget = ['wget','--user-agent={}'.format(user_agent),url,'-O','test']
  subprocess.run(wget,capture_output=False,check=True)
  a = subprocess.run(['sha1sum','test'],capture_output=True,check=True).stdout.decode('utf-8').split()
  b = subprocess.run(['sha1sum',fname],capture_output=True,check=True).stdout.decode('utf-8').split()
  Path('test'.format(dir)).unlink(missing_ok=True)
  if a[0] == b[0]:
    print("{}OK! {}{}".format(bcolors.OKGREEN,a[0],bcolors.ENDC))
    return []
  else:
    print("{}DIFFERENT! {} {}{}".format(bcolors.FAIL,a[0],b[0],bcolors.ENDC))
    return [fname]
  
def verifyEncryptedFile(url,key1,fname,key2):
  wget = ['wget','--user-agent={}'.format(user_agent),url,'-O','test']
  subprocess.run(wget,capture_output=False,check=True)
  iv = {}
  with open('base.crypt','wb') as enc:
    with open(fname,'rb') as src:
      s = src.read()
      # Drop the first 16 bytes from every ts file
      enc.write(s[16:])
      # But save the first 16 bytes, which is the IV needed for decryption
      iv['base']=(''.join('{:02x}'.format(x) for x in s[:16]))
  with open('comp.crypt','wb') as enc:
    with open('test','rb') as src:
      s = src.read()
      # Drop the first 16 bytes from every ts file
      enc.write(s[16:])
      # But save the first 16 bytes, which is the IV needed for decryption
      iv['comp']=(''.join('{:02x}'.format(x) for x in s[:16]))
  
  openssl1 = ['openssl','aes-128-cbc','-d','-in','base.crypt','-out','base','-nosalt','-K',key2,'-iv',iv['base']]
  openssl2 = ['openssl','aes-128-cbc','-d','-in','comp.crypt','-out','comp','-nosalt','-K',key1,'-iv',iv['comp']]
  print('{}{}{}'.format(bcolors.OKBLUE,openssl1,bcolors.ENDC))
  subprocess.run(openssl1,capture_output=False,check=True)
  print('{}{}{}'.format(bcolors.OKBLUE,openssl2,bcolors.ENDC))
  subprocess.run(openssl2,capture_output=False,check=True)
  a = subprocess.run(['sha1sum','base'],capture_output=True,check=True).stdout.decode('utf-8').split()
  b = subprocess.run(['sha1sum','comp'],capture_output=True,check=True).stdout.decode('utf-8').split()
  Path('test'.format(dir)).unlink(missing_ok=True)
  Path('base'.format(dir)).unlink(missing_ok=True)
  Path('base.crypt'.format(dir)).unlink(missing_ok=True)
  Path('comp'.format(dir)).unlink(missing_ok=True)
  Path('comp.crypt'.format(dir)).unlink(missing_ok=True)
  if a[0] == b[0]:
    print("{}OK! {}{}".format(bcolors.OKGREEN,a[0],bcolors.ENDC))
    return []
  else:
    print("{}DIFFERENT! {} {}{}".format(bcolors.FAIL,a[0],b[0],bcolors.ENDC))
    return [fname]

def verifyDownload(pgmdata):
  dir='{}_{}'.format(pgmdata['datecode'],pgmdata['name'])
  differences = []
  if pgmdata['captions'] != None:
    differences.extend(verifyFile(pgmdata['captions']['url'],'{}/{}'.format(dir,pgmdata['captions']['filename'])))
  if pgmdata['data']['mp4'] != None:
    differences.extend(verifyFile(pgmdata['data']['mp4']['url'],'{}/{}'.format(dir,pgmdata['data']['mp4']['filename'])))
  if pgmdata['data']['m3u'] != None:
    if pgmdata['data']['m3u']['encryption'] == None:
      for ts in pgmdata['data']['m3u']['playlist']:
        differences.extend(verifyFile(ts['url'],'{}/ts/{}'.format(dir,ts['file'])))
    else:
      # Encrypted
      req = urllib.request.Request(pgmdata['data']['m3u']['encryption']['keyurl'], data=None, headers=headers)
      with urllib.request.urlopen(req) as response:
        cryptokey = response.read()
      key=(''.join('{:02x}'.format(x) for x in cryptokey))
      for ts in pgmdata['data']['m3u']['playlist']:
        differences.extend(verifyEncryptedFile(ts['url'],key,'{}/ts/{}'.format(dir,ts['file']),pgmdata['data']['m3u']['encryption']['key'].replace(' ','')))
  
  #print(json.dumps(differences,indent=2,default=str))
  if len(differences)>0:
    print("{}DIFFERENCES FOUND.{}".format(bcolors.FAIL,bcolors.ENDC))
  else:
    print("{}OK{}".format(bcolors.OKGREEN,bcolors.ENDC))
  return differences

####################
### MAIN PROGRAM ###
####################

try:
  url = args.url[0]
except IndexError:
  url = ''


# Search for URLs online if requested to do so
if args.search:
  print('{}Searching for URLs from {} ...{}'.format(bcolors.OKBLUE,args.search,bcolors.ENDC))

  # Download webpage
  req = urllib.request.Request(args.search, data=None, headers=headers)
  with urllib.request.urlopen(req) as response:
    webpage = response.read()
    soup = BeautifulSoup(webpage,features='html.parser')
  #print(webpage.decode('utf-8'))

  links = []

  try:
    # Get PRELOAD (JSON)
    preload = json.loads(re.split(r'^PRELOAD\=(.+)$',soup.find('script',text=re.compile(r'^PRELOAD\=(.+)$')).string)[1])
    #print(json.dumps(preload,indent=2))
  
    try:
      slideshow = {}
      linkgroups = []
      shelves = []
      tiles = []
      show_id = ''
    
      deprecated=preload['deprecated'][list(preload['deprecated'].keys())[0]]
      for v in deprecated['episodes']['videos']:
        links.extend([{'url':v['permalink'],'date':v['airdate']}])
        if show_id=='':
          show_id = v['show']
      #print(json.dumps(deprecated['episodes']['videos'],indent=2))
      #print(show_id)
  
      apireq_fields = {
  'fields[videos]':'title,description,type,genre,vChipRating,vChipSubRatings,guid,published,runTime,airdate,available,seasonNumber,episodeNumber,expiration,entitlement,tveAuthWindow,nbcAuthWindow,externalAdId,uplynkStatus,dayPart,internalId,keywords,permalink,embedUrl,credits,selectedCountries,copyright',
  'fields[shows]':'active,category,colors,creditTypeLabel,description,frontends,genre,internalId,isCoppaCompliant,name,navigation,reference,schemaType,shortDescription,shortTitle,showTag,social,sortTitle,tuneIn,type,urlAlias',
  'fields[images]':'derivatives,path,width,attributes,altText,link',
  'fields[aggregatesShowProperties]':'videoEpisodeSeasons,videoTypes',
  'include':'image,show.image,show.aggregates',
  'filter[show]':show_id,
  'filter[available][value]':datetime.now().strftime('%Y-%m-%dT%H:%M:%S-05:00'),
  'filter[available][operator]':'<=',
  'filter[type][value]':'Full Episode',
  'filter[type][operator]':'=',
  'page[number]':'1',
  'page[size]':'100',
  'sort':'-airdate' }
      apiserver = re.split(r'(?:.*)API\:(?:[^\"]+)\"(https\:[^\"]+)\"(?:.*)',webpage.decode('utf-8'))[1] # https://api.nbc.com/v3.14
      #print(json.dumps(apiserver,indent=2))
      #print(webpage.decode('utf-8'))
      apiurl = '{}/videos?{}'.format(apiserver,'&'.join(['{}={}'.format(urllib.parse.quote(q),urllib.parse.quote(apireq_fields[q])) for q in apireq_fields]))
      #print(apiurl)
      req = urllib.request.Request(apiurl, data=None, headers=firefox_headers)
      with urllib.request.urlopen(req) as response:
        apidata = response.read()
      apijson = json.loads(apidata)
      for v in apijson['data']:
        links.extend([{'url':v['attributes']['permalink'],'date':v['attributes']['airdate']}])
      
      #print(apidata)
      #print(json.dumps(apijson,indent=2))
      
    except IndexError:
      pass
  
    try:
      slideshow = {}
      linkgroups = []
      shelves = []
      tiles = []
    
      preload_pages=preload['pages'][list(preload['pages'].keys())[0]]
      #print(json.dumps(preload_pages,indent=2))
      for p in preload_pages['data']['sections']:
        if p['component'] == 'Slideshow':
          slideshow=p
        if p['component'] == 'LinksSelectableGroup':
          linkgroups.extend([p])
      for l in linkgroups:
        for j in l['data']['items']:
          if j['component'] == 'Shelf':
            shelves.extend([j])
      for s in shelves:
        for t in s['data']['items']:
          if t['component'] == 'VideoTile' and t['data']['programmingType'] == 'Full Episode':
            tiles.extend([t])
      for t in tiles:
        links.extend([{'url':'https://www.nbc.com{}'.format(t['data']['permalink']),'date':t['data']['airDate']}])
    except IndexError:
      pass
  except AttributeError:
    pass


  try:
    nextdata = soup.find(attrs={'id':'__NEXT_DATA__'}).string
    jsondata = json.loads(nextdata)
    layouts = jsondata['props']['initialState']['front']['curation']['layouts'] #['packages']
    broadcasts = []
    for layout in layouts:
      for package in layout['packages']:
        #print({'name':package['name'],'type':package['type'],'zone':package['zone'],'items':len(package['items'])})
        #print(json.dumps(package,indent=2))
        if package['name'] == None:
          package['name'] = ''
        if ('kids' in args.search and package['type'] in ('bacon', 'oneUp')) or 'Full' in package['name']:
          broadcasts.extend(package['items'])
    #print(json.dumps(broadcasts,indent=2))
    for broadcast in broadcasts:
      try:
        links.extend([{'url':broadcast['item']['url']['canonical'],'date':broadcast['item']['dateBroadcast']}])
      except KeyError:
        pass
  except AttributeError:
    pass
    
  #print(json.dumps(links,indent=2))
  for link in links:
    try:
      broadcastDate = datetime.strptime(link['date'], '%Y-%m-%dT%H:%M:%S%z')
    except ValueError:
      try:
        broadcastDate = datetime.strptime(link['date'], '%Y-%m-%dT%H:%M:%S.%fZ')
      except ValueError:
        broadcastDate = datetime.strptime(link['date'], '%a %b %d %Y %H:%M:%S %Z%z (UTC)')
        broadcastDate = broadcastDate.replace(tzinfo=timezone(timedelta(seconds=-21600), 'CT')) - timedelta(seconds=21600)
    link['date'] = broadcastDate
    try:
      link['name'] = parseUrl(link['url'])['name']
      link['id'] = parseUrl(link['url'])['id']
    except IndexError:
      link['name'] = None
      link['id'] = None

  # Remove any without a name 
  for i in reversed(range(len(links))):
    if links[i]['id'] == None:
      links.pop(i)

  # Dedup the list
  links.sort(key=lambda x: x['id'])
  for i in reversed(range(1,len(links))):
    if links[i]['id'] == links[i-1]['id']:
      links.pop(i)
  links.sort(key=lambda x: x['date'],reverse=False)

  #for link in links:
  #  print('{} {} {}'.format(link['date'],link['name'],link['url']))

  # Organize by months and years
  yearmth = {}
  for link in links:
    year = (link['date']-timedelta(seconds=10800)).strftime('%Y')
    mnth = (link['date']-timedelta(seconds=10800)).strftime('%Y%m')
    if year not in yearmth.keys():
      yearmth[year] = {}
    if mnth not in yearmth[year].keys():
      yearmth[year][mnth] = []
    yearmth[year][mnth].extend([link])
  #print(json.dumps(yearmth,indent=2,default=str))

  # Organize by months and quarters
  yearqtr = {}
  for link in links:
    year = (link['date']-timedelta(seconds=10800)).strftime('%Y')
    qter = '{}{}'.format(year,quarter((link['date']-timedelta(seconds=10800)).strftime('%m')))
    if year not in yearqtr.keys():
      yearqtr[year] = {}
    if qter not in yearqtr[year].keys():
      yearqtr[year][qter] = []
    yearqtr[year][qter].extend([link])
  #print(json.dumps(yearqtr,indent=2,default=str))

  #Create output URL files
  if args.by_quarter:
    uu = yearqtr 
  else:
    uu = yearmth

  if args.searchout:
    od = args.searchout
    for year in list(uu.keys()):
      for sub in list(uu[year].keys()):
        pp = '{}/{}/{}'.format(od,year,sub)
        #print(pp)
        Path(pp).mkdir(parents=True,exist_ok=True)
        if Path('{}/url.txt'.format(pp)).is_file():
  
          with open('{}/url.txt'.format(pp),'r') as f:
            preexisting = f.read().splitlines()
          preexisting_p = [parseUrl(j) for j in preexisting]
          preexisting_ids = [j['id'] for j in preexisting_p]
          #print(json.dumps(preexisting_p,indent=2,default=str))
  
          to_add = []
          for j in uu[year][sub]:
            if j['id'] not in preexisting_ids:
              to_add.extend([j])
  
          #print(json.dumps(to_add,indent=2,default=str))
          if len(to_add)>0:
            if len(to_add)==1:
              print('{}Appending {:,} record to {}/url.txt ...{}'.format(bcolors.OKGREEN,len(to_add),pp,bcolors.ENDC))
            else:
              print('{}Appending {:,} records to {}/url.txt ...{}'.format(bcolors.OKGREEN,len(to_add),pp,bcolors.ENDC))
            with open('{}/url.txt'.format(pp),'w') as f:
              for k in preexisting:
                f.write('{}\n'.format(k))
              for k in to_add:
                f.write('{}\n'.format(k['url']))
  
          else:
            print('{}No new records for {}/url.txt ...{}'.format(bcolors.OKBLUE,pp,bcolors.ENDC))
  
        else:
          print('{}Writing {:,} record to {}/url.txt ...{}'.format(bcolors.OKGREEN,len(uu[year][sub]),pp,bcolors.ENDC))
          with open('{}/url.txt'.format(pp),'w') as f:
            for k in uu[year][sub]:
              f.write('{}\n'.format(k['url']))
  else:
    # Clean up the output a bit
    for y in list(uu.keys()):
      for s in list(uu[y].keys()):
        for i in uu[y][s]:
          i.pop('id')
          i.pop('name')
    # Print the URL list
    print(json.dumps(uu,indent=2,default=str))

  if len(uu)==0:
    print('{}NOTE: No URLs could be found at this address!{}'.format(bcolors.WARNING,bcolors.ENDC))
    sys.exit(90)
  else:
    sys.exit()


# If this is a restart run, then load the JSON.
if args.prevdir != '':
  with open('{}/metadata.json'.format(args.prevdir),'r') as j:
    pgmdata = json.load(j)
  if not (args.test or args.dryrun):
    ProcessData(pgmdata)

elif args.verify != '':
  with open('{}/metadata.json'.format(args.verify),'r') as j:
    pgmdata = json.load(j)
  #dir='{}_{}'.format(pgmdata['datecode'],pgmdata['name'])
  dir=args.verify
  differences = []
  if pgmdata['captions'] != None:
    differences.extend(verifyFile(pgmdata['captions']['url'],'{}/{}'.format(dir,pgmdata['captions']['filename'])))
  if pgmdata['data']['mp4'] != None:
    differences.extend(verifyFile(pgmdata['data']['mp4']['url'],'{}/{}'.format(dir,pgmdata['data']['mp4']['filename'])))
  if pgmdata['data']['m3u'] != None:
    if pgmdata['data']['m3u']['encryption'] == None:
      for ts in pgmdata['data']['m3u']['playlist']:
        differences.extend(verifyFile(ts['url'],'{}/ts/{}'.format(dir,ts['file'])))
    else:
      # Encrypted
      req = urllib.request.Request(pgmdata['data']['m3u']['encryption']['keyurl'], data=None, headers=headers)
      with urllib.request.urlopen(req) as response:
        cryptokey = response.read()
      key=(''.join('{:02x}'.format(x) for x in cryptokey))
      for ts in pgmdata['data']['m3u']['playlist']:
        differences.extend(verifyEncryptedFile(ts['url'],key,'{}/ts/{}'.format(dir,ts['file']),pgmdata['data']['m3u']['encryption']['key'].replace(' ','')))

  print(json.dumps(differences,indent=2,default=str))
  if len(differences)>0:
    print("{}DIFFERENCES FOUND.{}".format(bcolors.FAIL,bcolors.ENDC))
    sys.exit(99)
  else:
    print("{}OK{}".format(bcolors.OKGREEN,bcolors.ENDC))
    sys.exit(0)
  

else:
  if Path(url).is_file():
    skip = False
    mtime = datetime.fromtimestamp(Path(url).stat().st_mtime)
    age = (datetime.now() - mtime).total_seconds() / 3600.0
    if args.skiptime:
      try:
        if age > float(args.skiptime):
          print('{}File "{}" is {:,.2f} hours old, skipping processing.{}'.format(bcolors.WARNING,url,age,bcolors.ENDC))
          skip = True
      except ValueError:
        print('{}Invalid data supplied for option "-S"; please specify number of hours.{}'.format(bcolors.WARNING,bcolors.ENDC))
        skip = False
    if skip:
      urlfile = []
    else:
      with open(url,'r') as u:
        urlfile = u.read().splitlines()
  
  elif len(url)>1:
    urlfile = [url]

  else:
    urlfile = []
  
  for u in urlfile:
    print('{}{}{}'.format(bcolors.OKBLUE,u,bcolors.ENDC))
    pgmdata = CreateDownload(u)
    if not (args.test or args.dryrun or pgmdata == None) and args.securemode:
      dir='{}_{}'.format(pgmdata['datecode'],pgmdata['name'])
      if len(verifyDownload(pgmdata))>0:
        subprocess.run(['rm','-rf',dir],capture_output=False,check=True)
        pgmdata = CrateDownload(u)
        if len(verifyDownload(pgmdata))>0:
          subprocess.run(['rm','-rf',dir],capture_output=False,check=True)
          pgmdata = CrateDownload(u)
          if len(verifyDownload(pgmdata))>0:
            subprocess.run(['rm','-rf',dir],capture_output=False,check=True)
            pgmdata = None
    if not (args.test or args.dryrun):
      ProcessData(pgmdata)

print('{}Stats: {:,} downloads attempted, {:,} video files successful, {:,} links skipped.{}'.format(
	bcolors.HEADER,stats['attempted'],stats['success'],stats['skipped'],bcolors.ENDC))

retcd=0

if len(ambiguity)>0:
  print('{}The program encountered some ambiguity in program dates.'.format(bcolors.HEADER));
  print(json.dumps(ambiguity,indent=2,default=str))
  print(bcolors.ENDC)
  retcd=92

if stats['attempted'] > (stats['success'] + stats['skipped']):
  print('{}NOTE: There were some failures.{}'.format(bcolors.WARNING,bcolors.ENDC))
  retcd=91

sys.exit(retcd)
