#!/bin/env python3

import argparse
import json
import re
#import subprocess
from pathlib import Path
from datetime import datetime, timezone, timedelta
#import pytz
import urllib.request
from bs4 import BeautifulSoup

user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'

def import_m3u(m3ufile):
  m3uout = []
  item = {}
  for line in m3ufile:
    p1 = re.split(r'^#(.+):(.+)$',line)
    p2 = re.split(r'^#(.+):([\d\.]+),*$',line)
    p3 = re.split(r'^#(.+):(\d+),*$',line)
    if line in ('#EXTM3U' '#EXT-X-ENDLIST') or line.strip() == "":
      continue
    elif len(p3)==4:
      try:
        item[p3[1]] = int(p3[2])
      except:
        item[p3[1]] = p3[2]
    elif len(p2)==4:
      try:
        item[p2[1]] = float(p2[2])
      except:
        item[p2[1]] = p2[2]
    elif len(p1)==4: 
      ext3d = {}
      for p in re.findall(r'([^#:\,]+)=([^"\,]+|"[^"]*")',p1[2]):
        try:
          ext3d[p[0]]=int(p[1])
        except:
          ext3d[p[0]]=p[1]
      if len(ext3d)>0:
        item[p1[1]] = ext3d
      else:
        item[p1[1]] = p1[2]
    else:
      item['url']=line
      m3uout.extend([item])
      item = {}
  return m3uout

# Parse arguments
parser = argparse.ArgumentParser(description='Download/archive an NBC video stream.')
parser.add_argument('url',metavar='url',nargs=1,default='',
                    help='URL of video to download.')
parser.add_argument('-t','--test', action='store_true', dest='test',
                    help='Test mode, do not actually save any files.')
args = parser.parse_args()
url = args.url[0]


# Verify provided URL
link = re.split(r'(?x)^(?:https)?:?\/\/((?:www\.)?(?:nbcnews)\.com)\/([^\/]+)\/([^\/]+)\/((.+)-(.+))$',url)
type = 'nbcnews'

if len(link)<8:
  link = re.split(r'(?x)^(?:https)?:?\/\/((?:www\.)?(?:nbc)\.com)\/([^\/]+)\/([^\/]+)\/((.+)\/(.+))$',url)
  type = 'nbc'

clean_url = 'https://{}/{}/{}/{}'.format(link[1],link[2],link[3],link[4])

if len(link)<8:
  raise Exception("Unhandled URL pattern.")


# Download webpage
headers = {'User-Agent': user_agent}
req = urllib.request.Request(clean_url, data=None, headers=headers)
with urllib.request.urlopen(req) as response:
  webpage = response.read()
  soup = BeautifulSoup(webpage,features='html.parser')

nextdata = soup.find(attrs={'id':'__NEXT_DATA__'}).string
jsondata = json.loads(nextdata)
videoAssets = jsondata['props']['initialState']['video']['current']['videoAssets']
videoAssets.sort(key=lambda x: x['bitrate'],reverse=True)
videoAsset = videoAssets[0]['publicUrl']

videoTitle = jsondata['props']['initialState']['video']['current']['headline']['primary']
videoDesc = jsondata['props']['initialState']['video']['current']['description']['primary']
dateBroadcast = jsondata['props']['initialState']['video']['current']['dateBroadcast']
guid = jsondata['props']['initialState']['video']['current']['mpxMetadata']['guid']


# This converts the UTC to Central Time in a rudimentary way
# It would be better to use pyzt, but this calculation doesnt need to be
# really precise.  I am OK with it being off by an hour or two.
broadcastDate = datetime.strptime(dateBroadcast, '%a %b %d %Y %H:%M:%S %Z%z (UTC)')
broadcastDateCT = broadcastDate.replace(tzinfo=timezone(timedelta(seconds=-21600), 'CT')) - timedelta(seconds=21600)


# Get URL for closed captions (SRT) 
try:
  closedCaptioning = jsondata['props']['initialState']['video']['current']['closedCaptioning']
  closedCaptions = urllib.request.urlopen(closedCaptioning['srt']).geturl()
except:
  closedCaptions = None
#print(closedCaptions)


# Resolve link (MP4)
headers = {'User-Agent': user_agent}
req = urllib.request.Request(videoAsset, data=None, headers=headers)
with urllib.request.urlopen(req) as response:
 vdata1 = response.read()
 soup1 = BeautifulSoup(vdata1,features='html.parser')
xdata1 = [x.attrs for x in soup1.find_all('video')]
xdata1.sort(key=lambda x: x['system-bitrate'],reverse=True)
ydata1 = xdata1[0]['src']
mp4link=urllib.request.urlopen(ydata1).geturl()
#print(mp4link)


# Resolve link (M3U8)
headers = {'User-Agent': user_agent}
req = urllib.request.Request('{}&manifest=m3u'.format(videoAsset), data=None, headers=headers)
with urllib.request.urlopen(req) as response:
 vdata2 = response.read()
 soup2 = BeautifulSoup(vdata2,features='html.parser')
xdata2 = [x.attrs for x in soup2.find_all('video')]
ydata2 = xdata2[0]['src']
master_m3ulink=urllib.request.urlopen(ydata2).geturl()
#print(master_m3ulink)


# Download master.m3u8
headers = {'User-Agent': user_agent}
req = urllib.request.Request(master_m3ulink, data=None, headers=headers)
with urllib.request.urlopen(req) as response:
  master_m3u8 = response.read().decode('utf-8').split('\n')
m3ulist = import_m3u(master_m3u8)
m3ulist.sort(key=lambda x: x['EXT-X-STREAM-INF']['BANDWIDTH'],reverse=True)
m3uitem = m3ulist[0]
#print(json.dumps(m3uitem,indent=2))


# Download index.m3u8
headers = {'User-Agent': user_agent}
req = urllib.request.Request(m3uitem['url'], data=None, headers=headers)
with urllib.request.urlopen(req) as response:
  index_m3u8 = response.read().decode('utf-8').split('\n')
m3uindex = import_m3u(index_m3u8)
#print(json.dumps(m3uindex,indent=2))

# Determine the date code for the program
pgm_date = broadcastDateCT.strftime('%y%m%d')
chk_date = re.split(r'(?:.*)(\d{6})(?:.*)',guid)
if len(chk_date)>2:
  if pgm_date != chk_date[1]:
    raise Exception('ERROR: Ambiguity in program date between [{}] and {}.'.format(broadcastDateCT,guid))


# Assemble final program data
pgmdata = {}
pgmdata['datecode']=pgm_date
pgmdata['name']=link[4]
pgmdata['guid']=guid
pgmdata['title']=videoTitle
pgmdata['desc']=videoDesc
pgmdata['webpage']=clean_url
pgmdata['asset_url']=videoAsset
pgmdata['captions_url']=closedCaptions
pgmdata['mp4_url']=mp4link
pgmdata['master_url']=master_m3ulink
pgmdata['m3u8']=m3uitem

dir='{}_{}'.format(pgm_date,link[4])

if args.test:
  print(json.dumps(pgmdata,indent=2))

else:
  print(json.dumps(pgmdata,indent=2))
  print('Writing {}/response_data/webpage...'.format(dir))
  Path('{}/response_data'.format(dir)).mkdir(parents=True,exist_ok=True)
  with open('{}/response_data/webpage'.format(dir),'wb') as f:
    f.write(webpage)
  print('Writing {}/response_data/asset_mp4...'.format(dir))
  Path('{}/response_data'.format(dir)).mkdir(parents=True,exist_ok=True)
  with open('{}/response_data/asset_mp4'.format(dir),'wb') as f:
    f.write(vdata1)
  print('Writing {}/response_data/asset_m3u8...'.format(dir))
  Path('{}/response_data'.format(dir)).mkdir(parents=True,exist_ok=True)
  with open('{}/response_data/asset_m3u8'.format(dir),'wb') as f:
    f.write(vdata2)
