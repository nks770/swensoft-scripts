#!/bin/env python3

import argparse
import json
import demjson
import re
import subprocess
from pathlib import Path
from datetime import datetime, timezone, timedelta
#import pytz
import urllib.request, urllib.parse
from bs4 import BeautifulSoup

#user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'
#user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko/20100101 Firefox/84.0'
user_agent = 'Lavf/58.45.100'
headers = {'User-Agent': user_agent}

# Define terminal colors
class bcolors:
  HEADER = '\033[95m'
  OKBLUE = '\033[94m'
  OKGREEN = '\033[92m'
  WARNING = '\033[93m'
  FAIL = '\033[91m'
  ENDC = '\033[0m'
  BOLD = '\033[1m'
  UNDERLINE = '\033[4m'

def import_m3u(m3ufile):
  m3uout = []
  item = {}
  for line in m3ufile.decode('utf-8').split('\n'):
    p1 = re.split(r'^#(.+):(.+)$',line)
    p2 = re.split(r'^#(.+):([\d\.]+),*$',line)
    p3 = re.split(r'^#(.+):(\d+),*$',line)
    if line in ('#EXTM3U' '#EXT-X-ENDLIST') or line.strip() == "":
      continue
    elif len(p3)==4:
      try:
        item[p3[1]] = int(p3[2])
      except:
        item[p3[1]] = p3[2]
    elif len(p2)==4:
      try:
        item[p2[1]] = float(p2[2])
      except:
        item[p2[1]] = p2[2]
    elif len(p1)==4: 
      ext3d = {}
      for p in re.findall(r'([^#:\,]+)=([^"\,]+|"[^"]*")',p1[2]):
        try:
          ext3d[p[0]]=int(p[1])
        except:
          ext3d[p[0]]=p[1]
      if len(ext3d)>0:
        item[p1[1]] = ext3d
      else:
        item[p1[1]] = p1[2]
    else:
      item['url']=line
      m3uout.extend([item])
      item = {}
  return m3uout


def string_escape(s, encoding='utf-8'):
    return (s.encode('latin1')         # To bytes, required by 'unicode-escape'
             .decode('unicode-escape') # Perform the actual octal-escaping decode
             .encode('latin1')         # 1:1 mapping back to bytes
             .decode(encoding))        # Decode original encoding


def clean_bracket(s):
  b = 0
  for i in range(len(s)):
    if s[i]=='{':
      b=b+1
    if s[i]=='}':
      b=b-1
    if b==0:
      break
  return s[:i+1]

# Parse arguments
parser = argparse.ArgumentParser(description='Download/archive an NBC video stream.')
parser.add_argument('url',metavar='url',nargs=1,default='',
                    help='URL of video to download.')
parser.add_argument('-t','--test', action='store_true', dest='test',
                    help='Test mode, do not actually save any files.')
parser.add_argument('-f','--force', action='store_true', dest='force',
                    help='Overwrite data if already exists.')
args = parser.parse_args()
url = args.url[0]


# Verify provided URL
link = re.split(r'(?x)^(?:https)?:?\/\/((?:www\.)?(?:nbcnews)\.com)\/([^\/]+)\/([^\/]+)\/((.+)-(.+))$',url)
type = 'nbcnews'

if len(link)<8:
  link = re.split(r'(?x)^(?:https)?:?\/\/((?:www\.)?(?:nbc)\.com)\/([^\/]+)\/([^\/]+)\/((.+)\/(.+))$',url)
  type = 'nbc'

clean_url = 'https://{}/{}/{}/{}'.format(link[1],link[2],link[3],link[4])

if len(link)<8:
  raise Exception("Unhandled URL pattern.")

#print(link)

# Download webpage
req = urllib.request.Request(clean_url, data=None, headers=headers)
with urllib.request.urlopen(req) as response:
  webpage = response.read()
  soup = BeautifulSoup(webpage,features='html.parser')

# Initialize program data dictionary
pgmdata = {}
pgmdata['webpage']=clean_url

if type == 'nbcnews':
  # Get NEXT_DATA (JSON)
  nextdata = soup.find(attrs={'id':'__NEXT_DATA__'}).string
  jsondata = json.loads(nextdata)
  videoAssets = jsondata['props']['initialState']['video']['current']['videoAssets']
  videoAssets.sort(key=lambda x: x['bitrate'],reverse=True)
  
  dateBroadcast = jsondata['props']['initialState']['video']['current']['dateBroadcast']

  # This converts the UTC to Central Time in a rudimentary way
  # It would be better to use pyzt, but this calculation doesnt need to be
  # really precise.  I am OK with it being off by an hour or two.
  broadcastDate = datetime.strptime(dateBroadcast, '%a %b %d %Y %H:%M:%S %Z%z (UTC)')
  broadcastDateCT = broadcastDate.replace(tzinfo=timezone(timedelta(seconds=-21600), 'CT')) - timedelta(seconds=21600)

  # Determine the date code for the program
  pgmdata['datecode'] = broadcastDateCT.strftime('%y%m%d')

  pgmdata['name'] = link[4]
  pgmdata['guid'] = jsondata['props']['initialState']['video']['current']['mpxMetadata']['guid']
  pgmdata['title'] = jsondata['props']['initialState']['video']['current']['headline']['primary']
  pgmdata['desc'] = jsondata['props']['initialState']['video']['current']['description']['primary']
  pgmdata['thumb'] = jsondata['props']['initialState']['video']['current']['primaryImage']['url']['primary']

  # Get URL for closed captions (SRT) 
  try:
    closedCaptioning = jsondata['props']['initialState']['video']['current']['closedCaptioning']
    pgmdata['captions'] = urllib.request.urlopen(closedCaptioning['srt']).geturl()
  except:
    pgmdata['captions'] = None

  # Video asset URLs
  assetUrl = re.split(r'^([^\?]+)\?(.+)$',videoAssets[0]['publicUrl'])
  if len(assetUrl)>2:
    pgmdata['asset'] = {'base':assetUrl[1],
                  'ext': assetUrl[2]}
#                'mp4': '',
#                'm3u': 'manifest=m3u'}
  else:
    pgmdata['asset'] = {'base':videoAssets[0]['publicUrl'],
                  'ext': None}
#                'mp4': '',
#                'm3u': 'manifest=m3u'}

  # Check the date code
  chk_date = re.split(r'(?:.*)(\d{6})(?:.*)',pgmdata['guid'])
  if len(chk_date)>2:
    if pgmdata['datecode'] != chk_date[1]:
      raise Exception('ERROR: Ambiguity in program date between [{}] and {}.'.format(broadcastDateCT,pgmdata['guid']))

#  print(json.dumps(jsondata,indent=2))

if type == 'nbc':

  # Get LD+JSON (JSON)
  ldjson = json.loads(soup.find('script',attrs={'type':re.compile('json')}).string)

  # Get EXPORTS (JSON)
  exports = webpage.decode('utf-8').split('.exports=')
  for i in reversed(range(len(exports))):
    exports[i] = clean_bracket(exports[i])
    if exports[i][0] != '{' or 'MPX' not in exports[i]:
      exports.pop(i)
  expdata = re.sub(r'\!([01{1}])',r'\1',exports[0])
  jsondata = demjson.decode(expdata)

  # Get PRELOAD (JSON)
  preload = json.loads(re.split(r'^PRELOAD\=(.+)$',soup.find('script',text=re.compile(r'^PRELOAD\=(.+)$')).string)[1])
  preload_pages=preload['pages'][list(preload['pages'].keys())[0]]

  # Get data from player link
  plink = '{}/p/{}/{}/select/media/guid/{}/{}'.format(jsondata['MPX']['domain'],jsondata['MPX']['pid'],jsondata['MPX']['playerName'],jsondata['MPX']['id'],link[6])
#  print(plink)
  req = urllib.request.Request(plink, data=None, headers=headers)
  with urllib.request.urlopen(req) as response:
    pdata = response.read()
    psoup = BeautifulSoup(pdata,features='html.parser')
 
  # Determine videoAsset link
  plink2a = psoup.find('link',attrs={'rel':'alternate','type':re.compile('smil')})['href']
  plink2b = re.findall(r'(?i:tp\:releaseurl)=\"([^\"]+)\"',pdata.decode('utf-8'))[0]
  if plink2a == None:
    plink2a = plink2b
  if plink2a != None and plink2b != None and plink2a != plink2b:
    raise Exception("ERROR: Links different.\nlink1={}\nlink2={}.".format(plink2a,plink2b))

  previewUrl = '{}&format=preview&formats=MPEG-DASH+widevine,M3U+appleHlsEncryption,M3U+none,MPEG-DASH+none,MPEG4,MP3'.format(plink2a)
  req = urllib.request.Request(previewUrl, data=None, headers=headers)
  with urllib.request.urlopen(req) as response:
    preview = response.read()
  preview_json = json.loads(preview.decode('utf-8'))

#  print(psoup.decode('utf-8'))
#  print(json.dumps(ldjson,indent=2))
#  print(json.dumps(jsondata,indent=2))
#  print(json.dumps(preload_pages,indent=2))
#  print(json.dumps(preview_json,indent=2))

  # Broadcast date
  try:
    dateBroadcast = preload_pages['metadata']['airDate']
  except KeyError:
    dateBroadcast = ldjson['video']['uploadDate']

  # This converts the UTC to Central Time in a rudimentary way
  # It would be better to use pyzt, but this calculation doesnt need to be
  # really precise.  I am OK with it being off by an hour or two.
  broadcastDate = datetime.strptime(dateBroadcast, '%Y-%m-%dT%H:%M:%S.%fZ')
  broadcastDateCT = broadcastDate.replace(tzinfo=timezone(timedelta(seconds=-21600), 'CT')) - timedelta(seconds=21600)

  # Determine the date code for the program
  pgmdata['datecode'] = broadcastDateCT.strftime('%y%m%d')
  #chk_date = re.split(r'(?:.*)(\d{6})(?:.*)',guid)
  #if len(chk_date)>2:
  #  if datecode != chk_date[1]:
  #    raise Exception('ERROR: Ambiguity in program date between [{}] and {}.'.format(broadcastDateCT,guid))
  pgmdata['name'] = link[5]
  pgmdata['guid'] = link[5]

  # Video title
  try:
    pgmdata['title'] = ldjson['video']['name']
  except KeyError:
    try:
      pgmdata['title'] = preload_pages['metadata']['secondaryTitle']
    except KeyError:
      pgmdata['title'] = psoup.find('meta',attrs={'property':'og:title'})['content']

  # Video description
  try:
    pgmdata['desc'] = preload_pages['metadata']['ariaLabel']
  except KeyError:
    try:
      pgmdata['desc'] = preload_pages['metadata']['title']
    except KeyError:
      try:
        pgmdata['desc'] = preload_pages['metadata']['description']
      except KeyError:
        try:
          pgmdata['desc'] = ldjson['video']['description']
        except KeyError:
          pgmdata['desc'] = psoup.find('meta',attrs={'property':'og:description'})['content']

  # Webpage URLs
  pgmdata['webpage'] = clean_url

  # Thumbnail image URL
  try:
    pgmdata['thumb'] = ldjson['video']['thumbnailUrl']
  except KeyError:
    try:
      pgmdata['thumb'] = preload_pages['metadata']['image']
    except KeyError:
      pgmdata['thumb'] = psoup.find('meta',attrs={'property':'og:image'})['content']

  # Get the closed captions file
  pgmdata['captions'] = None
  for cap in preview_json['captions']:
    if cap['lang'] == 'en':
      pgmdata['captions'] = cap['src']

  # Video asset URLs
  assetUrl = re.split(r'^([^\?]+)\?(.+)$',plink2a)
  if len(assetUrl)>2:
    pgmdata['asset'] = {'base':assetUrl[1],
                  'ext': assetUrl[2]}
#                  'mp4': 'format=SMIL&formats=MPEG4',
#                  'm3u': 'format=SMIL&formats=MPEG-DASH+widevine,M3U+appleHlsEncryption,M3U+none,MPEG-DASH+none,MPEG4,MP3'}
  else:
    pgmdata['asset'] = {'base':plink2a,
                  'ext': None}
#                  'mp4': 'format=SMIL&formats=MPEG4',
#                  'm3u': 'format=SMIL&formats=MPEG-DASH+widevine,M3U+appleHlsEncryption,M3U+none,MPEG-DASH+none,MPEG4,MP3'}

  
pgmdata['data'] = {}
pgmdata['data']['mp4'] = {}
pgmdata['data']['m3u'] = {}

# Get MP4 file link
#url = '{}?mbr=true'.format(pgmdata['asset']['base'])
url = '{}?mbr=true&player=%5Bv2%5D%20OneApp%20-%20PDK6%20NBC.com&format=SMIL&formats=MPEG4'.format(pgmdata['asset']['base'])
req = urllib.request.Request(url, data=None, headers=headers)
with urllib.request.urlopen(req) as response:
 vdata1 = response.read()
 soup1 = BeautifulSoup(vdata1,features='html.parser')
#print(vdata1.decode('utf-8'))
xdata1 = [x.attrs for x in soup1.find_all('video')]
xdata1.sort(key=lambda x: int(x['system-bitrate']),reverse=True)
#print(json.dumps(xdata1,indent=2))
try:
  pgmdata['data']['mp4']['url'] = urllib.request.urlopen(xdata1[0]['src']).geturl()
  pgmdata['data']['mp4']['filename'] = re.split(r'^(.+)\/([^\/]+.mp4)(.*)$',pgmdata['data']['mp4']['url'])[2]
  try:
    pgmdata['data']['mp4']['width'] = int(xdata1[0]['width'])
    pgmdata['data']['mp4']['height'] = int(xdata1[0]['height'])
  except KeyError:
    pgmdata['data']['mp4']['width'] = None
    pgmdata['data']['mp4']['height'] = None
except (urllib.error.HTTPError, IndexError) as e:
  pgmdata['data']['mp4'] = None


# Get M3U file link
#url = '{}?mbr=true&manifest=m3u'.format(pgmdata['asset']['base'])
url = '{}?mbr=true&format=SMIL&manifest=m3u'.format(pgmdata['asset']['base'])
#url = '{}?player=%5Bv2%5D%20OneApp%20-%20PDK6%20NBC.com&format=SMIL&formats=MPEG-DASH+widevine,M3U+appleHlsEncryption,M3U+none,MPEG-DASH+none,MPEG4,MP3'.format(pgmdata['asset']['base'])
#url = '{}?player=%5Bv2%5D%20OneApp%20-%20PDK6%20NBC.com&manifest=m3u&format=SMIL&formats=MPEG-DASH+widevine,M3U+appleHlsEncryption,M3U+none,MPEG-DASH+none,MPEG4,MP3'.format(pgmdata['asset']['base'])
req = urllib.request.Request(url, data=None, headers=headers)
with urllib.request.urlopen(req) as response:
 vdata2 = response.read()
 soup2 = BeautifulSoup(vdata2,features='html.parser')
#print(vdata2.decode('utf-8'))
xdata2 = [x.attrs for x in soup2.find_all('video')]
xdata2.sort(key=lambda x: int(x['width']),reverse=True)
#print(json.dumps(xdata2[0],indent=2))
try:
  if len(xdata2[0]['abstract']) > len(pgmdata['desc']):
    pgmdata['desc'] = xdata2[0]['abstract']
except IndexError:
  pass
try:
  pgmdata['data']['m3u']['master'] = urllib.request.urlopen(xdata2[0]['src']).geturl()
except (urllib.error.HTTPError,IndexError) as e:
  pgmdata['data']['m3u'] = None


# Download master.m3u8
#print(pgmdata['data']['m3u']['master'])
req = urllib.request.Request(pgmdata['data']['m3u']['master'], data=None, headers=headers)
with urllib.request.urlopen(req) as response:
  vdata3 = response.read()
#print(vdata3.decode('utf-8'))
xdata3 = import_m3u(vdata3)
xdata3.sort(key=lambda x: x['EXT-X-STREAM-INF']['BANDWIDTH'],reverse=True)
#print(json.dumps(xdata3,indent=2))
pgmdata['data']['m3u']['index'] = xdata3[0]['url']
pgmdata['data']['m3u']['indexfile'] = re.split(r'^(.+)\/([^\?]+)(.*)$',pgmdata['data']['m3u']['index'])[2]
#print(json.dumps(xdata3[0],indent=2))
try:
  resolution=re.split(r'(\d+)x(\d+)',xdata3[0]['EXT-X-STREAM-INF']['RESOLUTION'])
  pgmdata['data']['m3u']['width'] = int(resolution[1])
  pgmdata['data']['m3u']['height'] = int(resolution[2])
except KeyError:
  pgmdata['data']['m3u']['width'] = None
  pgmdata['data']['m3u']['height'] = None


# Open index.m3u8
req = urllib.request.Request(pgmdata['data']['m3u']['index'], data=None, headers=headers)
with urllib.request.urlopen(req) as response:
  vdata4 = response.read()
#print(vdata4.decode('utf-8'))
xdata4 = import_m3u(vdata4)
print(json.dumps(xdata4,indent=2))

# Assemble final program data
print(json.dumps(pgmdata,indent=2))
quit()


#
## Download index.m3u8
#headers = {'User-Agent': user_agent}
#req = urllib.request.Request(m3uitem['url'], data=None, headers=headers)
#with urllib.request.urlopen(req) as response:
#  index_m3u8 = response.read().decode('utf-8').split('\n')
#m3uindex = import_m3u(index_m3u8)
##print(json.dumps(m3uindex,indent=2))
#


#pgmdata['m3u8']=m3uitem
#pgmdata['m3u8']['playlist']=re.split(r'^(.+)\/([^\?]+)(.*)$',m3uitem['url'])[2]

dir='{}_{}'.format(pgm_date,link[4])

file_exists = False
if Path('.archive/{}.zip'.format(dir)).is_file() or Path('{}.mp4'.format(dir)).is_file():
  file_exists = True

commands = []
if not args.test:
  if file_exists:
    if args.force:
      if Path('.archive/{}.zip'.format(dir)).is_file():
        commands.extend([['rm','-fv','.archive/{}.zip'.format(dir)]])
      if Path('{}.mp4'.format(dir)).is_file():
        commands.extend([['rm','-fv','{}.mp4'.format(dir)]])
    else:
      print("Nothing to do.")
      quit()

# Create list of shell commands
copycmd = []
cleancmd = []
archive = ['zip','-9r','.archive/{}.zip'.format(dir),dir]
rmcmd = ['rm','-rf',dir]

if pgmdata['master_url'] != None:
  commands.extend([['wget',pgmdata['master_url'],'-O','{}/ts/master.m3u8'.format(dir)]])
  commands.extend([['wget',m3uitem['url'],'-O','{}/ts/{}'.format(dir,pgmdata['m3u8']['playlist'])]])
  for tsfile in m3uindex:
    commands.extend([['wget',tsfile['url'],'-O','{}/ts/{}'.format(dir,re.split(r'^(.+)\/([^\?]+)(.*)$',tsfile['url'])[2])]])
  copycmd = ['ffmpeg','-i','{}/{}.ts'.format(dir,guid),'-acodec','copy','-vcodec','copy','{}.mp4'.format(dir)]
  cleancmd = ['rm','-fv','{}/{}.ts'.format(dir,guid)]

if pgmdata['mp4_url'] != None:
  commands.extend([['wget',pgmdata['mp4_url'],'-O','{}/{}'.format(dir,re.split(r'^(.+)\/([^\?]+)(.*)$',pgmdata['mp4_url'])[2])]])
  copycmd = ['cp','-av','{}/{}'.format(dir,re.split(r'^(.+)\/([^\?]+)(.*)$',pgmdata['mp4_url'])[2]),'{}.mp4'.format(dir)]
  cleancmd = []

if pgmdata['captions_url'] != None:
  commands.extend([['wget',pgmdata['captions_url'],'-O','{}/{}'.format(dir,re.split(r'^(.+)\/([^\?]+)(.*)$',pgmdata['captions_url'])[2])]])

if len(copycmd)>0:
  commands.extend([copycmd])
if len(cleancmd)>0:
  commands.extend([cleancmd])

commands.extend([archive,rmcmd])

if args.test:
  print(json.dumps(pgmdata,indent=2))

  for cmd in commands:
    print('{}{}{}'.format(bcolors.OKGREEN,cmd,bcolors.ENDC))

else:
  # Create directories
  Path('.archive').mkdir(parents=True,exist_ok=True)
  Path('{}/response_data'.format(dir)).mkdir(parents=True,exist_ok=True)
  if pgmdata['master_url'] != None:
    Path('{}/ts'.format(dir)).mkdir(parents=True,exist_ok=True)

  # Write data
  print('Writing {}/pgmdata.json...'.format(dir))
  with open('{}/pgmdata.json'.format(dir),'w') as f:
    json.dump(pgmdata,f)
  print('Writing {}/response_data/webpage...'.format(dir))
  with open('{}/response_data/webpage'.format(dir),'wb') as f:
    f.write(webpage)
  print('Writing {}/response_data/asset_mp4...'.format(dir))
  with open('{}/response_data/asset_mp4'.format(dir),'wb') as f:
    f.write(vdata1)
  print('Writing {}/response_data/asset_m3u8...'.format(dir))
  with open('{}/response_data/asset_m3u8'.format(dir),'wb') as f:
    f.write(vdata2)

  # Combine TS files, if required
  if copycmd[0] == 'ffmpeg':
    with open('{}/{}.ts'.format(guid),'wb') as out:
      for ts['url'] in m3uindex:
        with open('{}/ts/{}'.format(dir,re.split(r'^(.+)\/([^\?]+)(.*)$',tsfile['url'])[2]),'rb') as inp:
          out.write(inp.read())

  # Run commands
  for cmd in commands:
    #print('{}{}{}'.format(bcolors.OKGREEN,cmd,bcolors.ENDC))
    subprocess.run(cmd,capture_output=False,check=True)
